# Deep Research Pipeline Configuration - Parallelized Version
enable_cache: true

# Pipeline parameters
parameters:
  query: "What is Metaflow?"
  max_sub_questions: 10

# Step parameters
steps:
  initial_query_decomposition_step:
    parameters:
      sambanova_base_url: "https://api.sambanova.ai/v1"
      llm_model: "DeepSeek-R1-Distill-Llama-70B"
  
  process_sub_question_step:
    parameters:
      sambanova_base_url: "https://api.sambanova.ai/v1"
      llm_model_search: "Meta-Llama-3.3-70B-Instruct"
      llm_model_synthesis: "DeepSeek-R1-Distill-Llama-70B"
      num_results_per_search: 3
      cap_search_length: 20000
  
  merge_sub_question_results_step:
    parameters:
      step_prefix: "process_question_"
      output_name: "output"
  
  cross_viewpoint_analysis_step:
    parameters:
      sambanova_base_url: "https://api.sambanova.ai/v1"
      llm_model: "DeepSeek-R1-Distill-Llama-70B"
      viewpoint_categories: ["scientific", "political", "economic", "social", "ethical", "historical"]
  
  iterative_reflection_step:
    parameters:
      max_additional_searches: 2
      num_results_per_search: 3
      sambanova_base_url: "https://api.sambanova.ai/v1"
      llm_model: "DeepSeek-R1-Distill-Llama-70B"
  
  final_report_generation_step:
    parameters:
      sambanova_base_url: "https://api.sambanova.ai/v1"
      llm_model: "DeepSeek-R1-Distill-Llama-70B"

# Environment settings
settings:
  docker:
    requirements:
      - openai>=1.0.0
      - tavily-python>=0.2.8
      - PyYAML>=6.0
      - click>=8.0.0
      - pydantic>=2.0.0
      - typing_extensions>=4.0.0
  
  # Uncomment and customize these settings when running with orchestrators that support parallelization
  # orchestrator.kubeflow:
  #   synchronous: false
  #   resources:
  #     cpu_request: "1"
  #     memory_request: "2Gi"
  #     cpu_limit: "2"  
  #     memory_limit: "4Gi"
  
  # orchestrator.kubernetes:
  #   synchronous: false
  #   resources:
  #     process_sub_question_step:
  #       cpu_request: "1"
  #       memory_request: "2Gi" 