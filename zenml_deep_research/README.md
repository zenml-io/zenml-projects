# üîç ZenML Deep Research Agent

A production-ready MLOps pipeline for conducting deep, comprehensive research on any topic using LLMs and web search capabilities.

<div align="center">
  <img alt="Research Pipeline Visualization" src="assets/pipeline_visualization.png" width="70%">
  <p><em>ZenML Deep Research pipeline flow</em></p>
</div>

## üéØ Overview

The ZenML Deep Research Agent is a scalable, modular pipeline that automates in-depth research on any topic. It:

- Creates a structured outline based on your research query
- Researches each section through targeted web searches and LLM analysis
- Iteratively refines content through reflection cycles
- Produces a comprehensive, well-formatted research report
- Visualizes the research process and report structure in the ZenML dashboard

This project transforms exploratory notebook-based research into a production-grade, reproducible, and transparent process using the ZenML MLOps framework.

## üìù Example Research Results

The Deep Research Agent produces comprehensive, well-structured reports on any topic. Here's an example of research conducted on quantum computing:

<div align="center">
  <img alt="Sample Research Report" src="assets/sample_report.png" width="70%">
  <p><em>Sample report generated by the Deep Research Agent</em></p>
</div>

## üöÄ Pipeline Architecture

The pipeline uses a parallel processing architecture for efficiency and breaks down the research process into granular steps for maximum modularity and control:

1. **Query Decomposition**: Break down the main query into specific sub-questions
2. **Parallel Information Gathering**: Process multiple sub-questions concurrently for faster results
3. **Information Validation & Synthesis**: Validate sources, remove redundancies, and synthesize findings
4. **Cross-Viewpoint Analysis**: Analyze discrepancies and agreements between different perspectives
5. **Iterative Reflection with Human Approval**: Self-critique research output to identify gaps and optionally request human approval before conducting additional searches
6. **Final Report Generation**: Compile all synthesized information into a coherent report

This architecture enables:
- Better reproducibility and caching of intermediate results
- Parallel processing for faster research completion
- Easier debugging and monitoring of specific research stages
- More flexible reconfiguration of individual components
- Enhanced transparency into how the research is conducted
- Human oversight and control over iterative research expansions

## üí° Under the Hood

- **LLM Integration**: Uses litellm for flexible access to various LLM providers
- **Web Research**: Utilizes Tavily API for targeted internet searches
- **ZenML Orchestration**: Manages pipeline flow, artifacts, and caching
- **Reproducibility**: Track every step, parameter, and output via ZenML
- **Visualizations**: Interactive visualizations of the research structure and progress
- **Report Generation**: Uses static HTML templates for consistent, high-quality reports
- **Human-in-the-Loop**: Optional approval mechanism via ZenML alerters (Discord, Slack, etc.)

## üõ†Ô∏è Getting Started

### Prerequisites

- Python 3.9+
- ZenML installed and configured
- API key for your preferred LLM provider (configured with litellm)
- Tavily API key

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd zenml_deep_research

# Install dependencies
pip install -r requirements.txt

# Set up API keys
export OPENAI_API_KEY=your_openai_key  # Or another LLM provider key
export TAVILY_API_KEY=your_tavily_key  # For Tavily search (default)
export EXA_API_KEY=your_exa_key        # For Exa search (optional)

# Initialize ZenML (if needed)
zenml init
```

### Running the Pipeline

#### Basic Usage

```bash
# Run with default configuration
python run.py
```

The default configuration and research query are defined in `configs/enhanced_research.yaml`.

#### Using Research Mode Presets

The pipeline includes three pre-configured research modes for different use cases:

```bash
# Rapid mode - Quick overview with minimal depth
python run.py --mode rapid

# Balanced mode - Standard research depth (default)
python run.py --mode balanced

# Deep mode - Comprehensive analysis with maximum depth
python run.py --mode deep
```

**Mode Comparison:**

| Mode | Sub-Questions | Search Results* | Additional Searches | Best For |
|------|---------------|----------------|-------------------|----------|
| **Rapid** | 5 | 2 per search | 0 | Quick overviews, time-sensitive research |
| **Balanced** | 10 | 3 per search | 2 | Most research tasks, good depth/speed ratio |
| **Deep** | 15 | 5 per search | 4 | Comprehensive analysis, academic research |

*Can be overridden with `--num-results`

#### Using Different Configurations

```bash
# Run with a custom configuration file
python run.py --config configs/custom_enhanced_config.yaml

# Override the research query from command line
python run.py --query "My research topic"

# Specify maximum number of sub-questions to process in parallel
python run.py --max-sub-questions 15

# Combine mode with other options
python run.py --mode deep --query "Complex topic" --require-approval

# Combine multiple options
python run.py --config configs/custom_enhanced_config.yaml --query "My research topic" --max-sub-questions 12
```

### Advanced Options

```bash
# Enable debug logging
python run.py --debug

# Disable caching for a fresh run
python run.py --no-cache

# Specify a log file
python run.py --log-file research.log

# Enable human-in-the-loop approval for additional research
python run.py --require-approval

# Set approval timeout (in seconds)
python run.py --require-approval --approval-timeout 7200

# Use a different search provider (default: tavily)
python run.py --search-provider exa                      # Use Exa search
python run.py --search-provider both                     # Use both providers
python run.py --search-provider exa --search-mode neural # Exa with neural search

# Control the number of search results per query
python run.py --num-results 5                            # Get 5 results per search
python run.py --num-results 10 --search-provider exa     # 10 results with Exa
```

### Search Providers

The pipeline supports multiple search providers for flexibility and comparison:

#### Available Providers

1. **Tavily** (Default)
   - Traditional keyword-based search
   - Good for factual information and current events
   - Requires `TAVILY_API_KEY` environment variable

2. **Exa**
   - Neural search engine with semantic understanding
   - Better for conceptual and research-oriented queries
   - Supports three search modes:
     - `auto` (default): Automatically chooses between neural and keyword
     - `neural`: Semantic search for conceptual understanding
     - `keyword`: Traditional keyword matching
   - Requires `EXA_API_KEY` environment variable

3. **Both**
   - Runs searches on both providers
   - Useful for comprehensive research or comparing results
   - Requires both API keys

#### Usage Examples

```bash
# Use Exa with neural search
python run.py --search-provider exa --search-mode neural

# Compare results from both providers
python run.py --search-provider both

# Use Exa with keyword search for exact matches
python run.py --search-provider exa --search-mode keyword

# Combine with other options
python run.py --mode deep --search-provider exa --require-approval
```

### Human-in-the-Loop Approval

The pipeline supports human approval for additional research queries identified during the reflection phase:

```bash
# Enable approval with default 1-hour timeout
python run.py --require-approval

# Custom timeout (2 hours)
python run.py --require-approval --approval-timeout 7200

# Approval works with any configuration
python run.py --config configs/thorough_research.yaml --require-approval
```

When enabled, the pipeline will:
1. Pause after the initial research phase
2. Send an approval request via your configured ZenML alerter (Discord, Slack, etc.)
3. Present research progress, identified gaps, and proposed additional queries
4. Wait for your approval before conducting additional searches
5. Continue with approved queries or finalize the report based on your decision

**Note**: You need a ZenML stack with an alerter configured (e.g., Discord or Slack) for approval functionality to work.

**Tip**: When using `--mode deep`, the pipeline will suggest enabling `--require-approval` for better control over the comprehensive research process.

## üìä Visualizing Research Process

The pipeline includes built-in visualizations to help you understand and monitor the research process:

### Viewing Visualizations

After running the pipeline, you can view the visualizations in the ZenML dashboard:

1. Start the ZenML dashboard:
   ```bash
   zenml up
   ```

2. Navigate to the "Runs" tab in the dashboard
3. Select your pipeline run
4. Explore visualizations for each step:
   - **report_structure_step**: View the initial report structure and outline
   - **paragraph_research_step**: See the research progress for each paragraph
   - **report_formatting_step**: View the final formatted report

### Visualization Features

The visualizations provide:
- An overview of the report structure
- Details of each paragraph's research status
- Search history and source information
- Progress through reflection iterations
- Professionally formatted HTML reports with static templates

### Sample Visualization

Here's what the report structure visualization looks like:

```
Report Structure:
‚îú‚îÄ‚îÄ Introduction
‚îÇ   ‚îî‚îÄ‚îÄ Initial understanding of the topic
‚îú‚îÄ‚îÄ Historical Background
‚îÇ   ‚îî‚îÄ‚îÄ Evolution and key developments
‚îú‚îÄ‚îÄ Current State
‚îÇ   ‚îî‚îÄ‚îÄ Latest advancements and implementations
‚îî‚îÄ‚îÄ Conclusion
    ‚îî‚îÄ‚îÄ Summary and future implications
```

## üìÅ Project Structure

```
zenml_deep_research/
‚îú‚îÄ‚îÄ configs/                    # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ enhanced_research.yaml  # Main configuration file
‚îú‚îÄ‚îÄ materializers/             # Custom materializers for artifact storage
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ pydantic_materializer.py 
‚îú‚îÄ‚îÄ pipelines/                 # ZenML pipeline definitions
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ parallel_research_pipeline.py
‚îú‚îÄ‚îÄ steps/                     # ZenML pipeline steps
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ approval_step.py         # Human approval step for additional research
‚îÇ   ‚îú‚îÄ‚îÄ cross_viewpoint_step.py
‚îÇ   ‚îú‚îÄ‚îÄ execute_approved_searches_step.py  # Execute approved searches
‚îÇ   ‚îú‚îÄ‚îÄ generate_reflection_step.py         # Generate reflection without execution
‚îÇ   ‚îú‚îÄ‚îÄ iterative_reflection_step.py       # Legacy combined reflection step
‚îÇ   ‚îú‚îÄ‚îÄ merge_results_step.py
‚îÇ   ‚îú‚îÄ‚îÄ process_sub_question_step.py
‚îÇ   ‚îú‚îÄ‚îÄ pydantic_final_report_step.py
‚îÇ   ‚îî‚îÄ‚îÄ query_decomposition_step.py
‚îú‚îÄ‚îÄ utils/                      # Utility functions and helpers
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ approval_utils.py       # Human approval utilities
‚îÇ   ‚îú‚îÄ‚îÄ helper_functions.py
‚îÇ   ‚îú‚îÄ‚îÄ llm_utils.py            # LLM integration utilities 
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py              # Contains prompt templates and HTML templates
‚îÇ   ‚îú‚îÄ‚îÄ pydantic_models.py      # Data models using Pydantic
‚îÇ   ‚îî‚îÄ‚îÄ search_utils.py         # Web search functionality
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ requirements.txt           # Project dependencies
‚îú‚îÄ‚îÄ logging_config.py          # Logging configuration
‚îú‚îÄ‚îÄ README.md                  # Project documentation
‚îî‚îÄ‚îÄ run.py                     # Main script to run the pipeline
```

## üîß Customization

The project supports two levels of customization:

### 1. Command-Line Parameters

You can customize the research behavior directly through command-line parameters:

```bash
# Specify your research query
python run.py --query "Your research topic"

# Control parallelism with max-sub-questions
python run.py --max-sub-questions 15

# Combine multiple options
python run.py --query "Your research topic" --max-sub-questions 12 --no-cache
```

These settings control how the parallel pipeline processes your research query.

### 2. Pipeline Configuration

For more detailed settings, modify the configuration file:

```yaml
# configs/enhanced_research.yaml

# Enhanced Deep Research Pipeline Configuration
enable_cache: true

# Research query parameters
query: "Climate change policy debates"

# Step configurations
steps:
  initial_query_decomposition_step:
    parameters:
      llm_model: "sambanova/DeepSeek-R1-Distill-Llama-70B"
  
  cross_viewpoint_analysis_step:
    parameters:
      llm_model: "sambanova/DeepSeek-R1-Distill-Llama-70B"
      viewpoint_categories: ["scientific", "political", "economic", "social", "ethical", "historical"]
  
  iterative_reflection_step:
    parameters:
      llm_model: "sambanova/DeepSeek-R1-Distill-Llama-70B"
      max_additional_searches: 2
      num_results_per_search: 3
  
  # Human approval configuration (when using --require-approval)
  get_research_approval_step:
    parameters:
      timeout: 3600  # 1 hour timeout for approval
      max_queries: 2  # Maximum queries to present for approval
  
  pydantic_final_report_step:
    parameters:
      llm_model: "sambanova/DeepSeek-R1-Distill-Llama-70B"

# Environment settings
settings:
  docker:
    requirements:
      - openai>=1.0.0
      - tavily-python>=0.2.8
      - PyYAML>=6.0
      - click>=8.0.0
      - pydantic>=2.0.0
      - typing_extensions>=4.0.0 
```

To use a custom configuration file:

```bash
python run.py --config configs/custom_research.yaml
```

### Available Configurations

**Mode-Based Configurations** (automatically selected when using `--mode`):

| Config File | Mode | Description |
|-------------|------|-------------|
| `rapid_research.yaml` | `--mode rapid` | Quick overview with minimal depth |
| `balanced_research.yaml` | `--mode balanced` | Standard research with moderate depth |
| `deep_research.yaml` | `--mode deep` | Comprehensive analysis with maximum depth |

**Specialized Configurations:**

| Config File | Description | Key Parameters |
|-------------|-------------|----------------|
| `enhanced_research.yaml` | Default research configuration | Standard settings, 2 additional searches |
| `thorough_research.yaml` | In-depth analysis | 12 sub-questions, 5 results per search |
| `quick_research.yaml` | Faster results | 5 sub-questions, 2 results per search |
| `daily_trends.yaml` | Research on recent topics | 24-hour search recency, disable cache |
| `compare_viewpoints.yaml` | Focus on comparing perspectives | Extended viewpoint categories |
| `parallel_research.yaml` | Optimized for parallel execution | Configured for distributed orchestrators |

You can create additional configuration files by copying and modifying the base configuration files above.

## üìà Example Use Cases

- **Academic Research**: Rapidly generate preliminary research on academic topics
- **Business Intelligence**: Stay informed on industry trends and competitive landscape
- **Content Creation**: Develop well-researched content for articles, blogs, or reports
- **Decision Support**: Gather comprehensive information for informed decision-making

## üîÑ Integration Possibilities

This pipeline can integrate with:

- **Document Storage**: Save reports to database or document management systems
- **Web Applications**: Power research functionality in web interfaces
- **Alerting Systems**: Schedule research on key topics and receive regular reports
- **Other ZenML Pipelines**: Chain with downstream analysis or processing

## üìÑ License

This project is licensed under the Apache License 2.0.
