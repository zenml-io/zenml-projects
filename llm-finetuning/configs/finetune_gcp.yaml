# environment configuration
settings:
  docker:
    requirements: requirements.txt
    python_package_installer: "uv"

model:
  name: "peft-lora-zencoder15B-personal-copilot"
  description: "Fine-tuned `starcoder15B-personal-copilot-A100-40GB-colab` for ZenML pipelines."
  audience: "Data Scientists / ML Engineers"
  use_cases: "Code Generation for ZenML MLOps pipelines."
  limitations: "There is no guarantee that this model will work for your use case. Please test it thoroughly before using it in production."
  trade_offs: "This model is optimized for ZenML pipelines. It is not optimized for other libraries."
  ethics: "This model is trained on public data. Please use it responsibly."
  tags:
    - llm
    - peft
    - qlora
    - starcoder-15b

steps:
  trainer:
    experiment_tracker: zenml_wandb
    step_operator: vertex-us
    settings:
      step_operator.vertex:
        machine_type: "a2-highgpu-1g"
        accelerator_type: "NVIDIA_TESLA_A100"
        accelerator_count: 1
    parameters:
      args:
        model_path: "bigcode/starcoder"
        dataset_name: "zenml/zenml-codegen-v1"
        subset: "data"
        data_column: "content"
        split: "train"
        seq_length: 2048
        max_steps: 1000
        batch_size: 4
        gradient_accumulation_steps: 2
        learning_rate: 0.0002
        weight_decay: 0.01
        num_warmup_steps: 30
        eval_freq: 100
        save_freq: 100
        log_freq: 25
        num_workers: 4
        bf16: true
        no_fp16: false
        output_dir: "peft-lora-zencoder15B-personal-copilot"
        fim_rate: 0.5
        fim_spm_rate: 0.5
        use_peft_lora: true
        lora_r: 8
        lora_alpha: 32
        lora_dropout: 0.1
        lora_target_modules: "c_proj,c_attn,q_attn,c_fc,c_proj"
        use_flash_attn: true
        use_4bit_qunatization: true
        use_nested_quant: true
        bnb_4bit_compute_dtype: "bfloat16"
        output_peft_repo_id: "zenml/peft-lora-zencoder15B-personal-copilot"
