# environment configuration
settings:
  docker:
    requirements: requirements.txt

model:
  name: "peft-lora-zencoder15B-personal-copilot"
  version: production

steps:
  deploy_model_to_hf_hub:
    parameters:
      hf_endpoint_cfg:
        framework: pytorch
        task: text-generation
        accelerator: gpu
        vendor: aws
        region: us-east-1
        max_replica: 1
        instance_size: xlarge
        instance_type: p4de
        namespace: zenml
        custom_image:
          health_route: /health
          env:
            MAX_BATCH_PREFILL_TOKENS: "2048"
            MAX_INPUT_LENGTH: "1024"
            MAX_TOTAL_TOKENS: "1512"
            QUANTIZE: bitsandbytes
            MODEL_ID: /repository
          url: registry.internal.huggingface.tech/api-inference/community/text-generation-inference:sha-564f2a3
