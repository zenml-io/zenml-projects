# Deep Research Pipeline Configuration - Balanced Mode
enable_cache: true

# ZenML MCP
model:
  name: "deep_research"
  description: "Parallelized ZenML pipelines for deep research on a given query."
  tags:
    [
      "research",
      "exa",
      "tavily",
      "openrouter",
      "sambanova",
      "langfuse",
      "balanced",
    ]
  use_cases: "Research on a given query."

# Langfuse project name for LLM tracking
langfuse_project_name: "deep-research"

# Research parameters for balanced research
parameters:
  query: "Default research query"
  
steps:
  initial_query_decomposition_step:
    parameters:
      llm_model: "sambanova/DeepSeek-R1-Distill-Llama-70B"
      max_sub_questions: 10  # Balanced number of sub-questions
  
  process_sub_question_step:
    parameters:
      llm_model_search: "sambanova/Meta-Llama-3.3-70B-Instruct"
      llm_model_synthesis: "sambanova/DeepSeek-R1-Distill-Llama-70B"
      cap_search_length: 20000  # Standard cap for search length
  
  cross_viewpoint_analysis_step:
    parameters:
      llm_model: "sambanova/DeepSeek-R1-Distill-Llama-70B"
      viewpoint_categories:
        [
          "scientific",
          "political",
          "economic",
          "social",
          "ethical",
          "historical",
        ]  # Standard viewpoints
  
  generate_reflection_step:
    parameters:
      llm_model: "sambanova/DeepSeek-R1-Distill-Llama-70B"
  
  get_research_approval_step:
    parameters:
      timeout: 3600  # 1 hour timeout
      max_queries: 2  # Moderate additional queries
  
  execute_approved_searches_step:
    parameters:
      llm_model: "sambanova/Meta-Llama-3.3-70B-Instruct"
      cap_search_length: 20000
      
  pydantic_final_report_step:
    parameters:
      llm_model: "sambanova/DeepSeek-R1-Distill-Llama-70B"
      
# Environment settings
settings:
  docker:
    requirements:
      - zenml>=0.83.1
      - tavily-python>=0.2.8
      - exa-py>=1.0.0
      - PyYAML>=6.0
      - click>=8.0.0
      - pydantic>=2.0.0
      - typing_extensions>=4.0.0
      - requests
      - anthropic>=0.52.2
      - litellm==1.69.1
      - langfuse==2.60.8