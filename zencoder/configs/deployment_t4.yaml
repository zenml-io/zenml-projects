# environment configuration
settings:
  docker:
    requirements: requirements.txt

model_version:
  name: "peft-lora-zencoder15B-personal-copilot"
  version: production

steps:
  deploy_model_to_hf_hub:
    parameters:
      framework: pytorch
      task: text-generation
      accelerator: gpu
      vendor: aws
      region: us-east-1
      max_replica: 1
      instance_size: large
      instance_type: g4dn.12xlarge
      namespace: zenml
      custom_image:
        health_route: /health
        env:
          MAX_BATCH_PREFILL_TOKENS: "2048"
          MAX_INPUT_LENGTH: "1024"
          MAX_TOTAL_TOKENS: "1512"
          QUANTIZE: bitsandbytes
          MODEL_ID: /repository
        url: registry.internal.huggingface.tech/api-inference/community/text-generation-inference:sha-564f2a3