{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ab391a",
   "metadata": {},
   "source": [
    "# üåç Overview\n",
    "\n",
    "This demo is a minimalistic MLOps project intended to showcase how to put ML workflows in production. It features: \n",
    "\n",
    "- A feature engineering pipeline that loads data and prepares it for training.\n",
    "- A training pipeline that loads the preprocessed dataset and trains a model.\n",
    "- A batch inference pipeline that runs predictions on the trained model with new data.\n",
    "- A stack switching and leveraging of Sagemaker step operator to outsource training to Cloud\n",
    "- An analysis of training artifacts and their lineage (including connection with W&B)\n",
    "\n",
    "<img src=\"_assets/pipeline_overview.png\" width=\"50%\" alt=\"Pipelines Overview\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2977c",
   "metadata": {},
   "source": [
    "# üë∂ Step 0. Install Requirements\n",
    "\n",
    "Let's install ZenML to get started. First we'll install the latest version of\n",
    "ZenML as well as the `sklearn` and `xgboost` integration of ZenML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -r requirements.txt\n",
    "! zenml integration install sklearn xgboost -y\n",
    "! zenml connect --url https://1cf18d95-zenml.cloudinfra.zenml.io \n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ZenML and set the default stack\n",
    "!zenml init\n",
    "!zenml stack set local-sagemaker-step-operator-wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f775f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the imports at the top\n",
    "from zenml import Model\n",
    "from zenml.client import Client\n",
    "from zenml.logger import get_logger\n",
    "\n",
    "from pipelines import training, inference\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Initialize the ZenML client to fetch objects from the ZenML Server\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b474",
   "metadata": {},
   "source": [
    "# ‚åö Step 1: Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87909827",
   "metadata": {},
   "source": [
    "Now that we have our data it makes sense to train some models to get a sense of\n",
    "how difficult the task is. The Breast Cancer dataset is sufficiently large and complex \n",
    "that it's unlikely we'll be able to train a model that behaves perfectly since the problem \n",
    "is inherently complex, but we can get a sense of what a reasonable baseline looks like.\n",
    "\n",
    "We'll start with two simple models, a SGD Classifier and a Random Forest\n",
    "Classifier, both batteries-included from `sklearn`. We'll train them both on the\n",
    "same data and then compare their performance.\n",
    "\n",
    "<img src=\"_assets/training_pipeline.png\" width=\"50%\" alt=\"Training pipeline\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc08aecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m# Apache Software License 2.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Copyright (c) ZenML GmbH 2024. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# you may not use this file except in compliance with the License.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# You may obtain a copy of the License at\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# http://www.apache.org/licenses/LICENSE-2.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Unless required by applicable law or agreed to in writing, software\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArtifactConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_materializer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSagemakerMaterializer\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_materializers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSagemakerMaterializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_trn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mArtifactConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"breast_cancer_classifier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_model_artifact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Configure and train a model on the training dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This is an example of a model training step that takes in a dataset artifact\u001b[0m\n",
      "\u001b[0;34m    previously loaded and pre-processed by other steps in your pipeline, then\u001b[0m\n",
      "\u001b[0;34m    configures and trains a model on it. The model is then returned as a step\u001b[0m\n",
      "\u001b[0;34m    output artifact.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        dataset_trn: The preprocessed train dataset.\u001b[0m\n",
      "\u001b[0;34m        model_type: The type of model to train.\u001b[0m\n",
      "\u001b[0;34m        target: The name of the target column in the dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m        The trained model artifact.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m        ValueError: If the model type is not supported.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Initialize the model with the hyperparameters indicated in the step\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# parameters and train it on the training set.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"xgboost\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown model type {model_type}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training model {model}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_trn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_trn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at model training step\n",
    "%pycat steps/model_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a00008",
   "metadata": {},
   "source": [
    "Our two training steps both return different kinds of classifier\n",
    "models, so we use the generic `ClassifierMixin` type hint for the return type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f22174",
   "metadata": {},
   "source": [
    "ZenML allows you to load any version of any dataset that is tracked by the framework\n",
    "directly into a pipeline using the `Client().get_artifact_version` interface. This is very convenient\n",
    "in this case, as we'd like to send our preprocessed dataset from the older pipeline directly\n",
    "into the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01162d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m# Apache Software License 2.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Copyright (c) ZenML GmbH 2024. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# you may not use this file except in compliance with the License.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# You may obtain a copy of the License at\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# http://www.apache.org/licenses/LICENSE-2.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Unless required by applicable law or agreed to in writing, software\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0muuid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUUID\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_evaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_promoter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfeature_engineering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrain_dataset_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUUID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtest_dataset_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUUID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"target\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Model training pipeline.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This is a pipeline that loads the data from a preprocessing pipeline,\u001b[0m\n",
      "\u001b[0;34m    trains a model on it and evaluates the model. If it is the first model\u001b[0m\n",
      "\u001b[0;34m    to be trained, it will be promoted to production. If not, it will be\u001b[0m\n",
      "\u001b[0;34m    promoted only if it has a higher accuracy than the current production\u001b[0m\n",
      "\u001b[0;34m    model version.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        train_dataset_id: ID of the train dataset produced by feature engineering.\u001b[0m\n",
      "\u001b[0;34m        test_dataset_id: ID of the test dataset produced by feature engineering.\u001b[0m\n",
      "\u001b[0;34m        target: Name of target column in dataset.\u001b[0m\n",
      "\u001b[0;34m        model_type: The type of model to train.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Link all the steps together by calling them and passing the output\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# of one step as the input of the next step.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Execute Feature Engineering Pipeline\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mtrain_dataset_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtest_dataset_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_tst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_trn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_artifact_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mname_id_or_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset_id\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_tst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_artifact_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mname_id_or_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset_id\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_trn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_evaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_trn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_trn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_tst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_tst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_promoter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at training pipeline\n",
    "%pycat pipelines/training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b70fd3",
   "metadata": {},
   "source": [
    "The end goal of this quick baseline evaluation is to understand which of the two\n",
    "models performs better. We'll use the `evaluator` step to compare the two\n",
    "models. This step takes in the model from the trainer step, and computes its score\n",
    "over the testing set.\n",
    "\n",
    "Soon you will see that it is relatively easy to train ML models using ZenML pipelines. But it can be somewhat clunky to track\n",
    "all the models produced as you develop your experiments and use-cases. Luckily, ZenML offers a *Model Control Plane*,\n",
    "which is a central register of all your ML models.\n",
    "\n",
    "You can easily create a ZenML Model and associate it with your pipelines using the `Model` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_settings = {}\n",
    "\n",
    "# Lets add some metadata to the model to make it identifiable\n",
    "pipeline_settings[\"model\"] = Model(\n",
    "    name=\"breast_cancer_classifier\",\n",
    "    license=\"Apache 2.0\",\n",
    "    description=\"A breast cancer classifier\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64885ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the XGBoost model and tag the version name with \"xgboost\"\n",
    "pipeline_settings[\"model\"].tags = [\"breast_cancer\", \"classifier\", \"xgboost\"]\n",
    "\n",
    "# Use an XGBoost model with fixed seed.\n",
    "training.with_options(enable_cache=False,**pipeline_settings)(\n",
    "    model_type=\"xgboost\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgboost_run = client.get_pipeline(\"training\").last_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the SGD model and tag the version name with \"sgd\"\n",
    "pipeline_settings[\"model\"].tags = [\"breast_cancer\", \"classifier\", \"sgd\"]\n",
    "\n",
    "# Use a SGD classifier\n",
    "sgd_run = training.with_options(enable_cache=False,**pipeline_settings)(\n",
    "    model_type=\"sgd\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "sgd_run = client.get_pipeline(\"training\").last_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1a68a",
   "metadata": {},
   "source": [
    "You can see from the logs already how our model training went: the\n",
    "`XGBClassifier` performed considerably better than the `SGDClassifier`.\n",
    "We can use the ZenML `Client` to verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95810b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluator returns a float value with the accuracy\n",
    "xgboost_run.steps[\"model_evaluator\"].output.load() >= sgd_run.steps[\"model_evaluator\"].output.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09597223",
   "metadata": {},
   "source": [
    "Running both pipelines has created two associated **model versions**.\n",
    "You can list your ZenML model and their versions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb25913",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenml_model = client.get_model(\"breast_cancer_classifier\")\n",
    "print(zenml_model)\n",
    "\n",
    "versions = zenml_model.versions\n",
    "\n",
    "print(f\"Model {zenml_model.name} has {len(versions)} versions\")\n",
    "\n",
    "versions[-2].version, versions[-1].version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cfac2",
   "metadata": {},
   "source": [
    "The interesting part is that ZenML went ahead and linked all artifacts produced by the\n",
    "pipelines to that model version, including the two pickle files that represent our\n",
    "SGD and RandomForest classifier. We can see all artifacts directly from the model\n",
    "version object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31211413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the XGBoost version\n",
    "xgboost_zenml_model_version = client.list_model_versions(\"breast_cancer_classifier\", tag=\"xgboost\")[-1]\n",
    "\n",
    "# We can now load our classifier directly as well\n",
    "xgboost_classifier = xgboost_zenml_model_version.get_artifact(\"breast_cancer_classifier\").load()\n",
    "\n",
    "xgboost_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53517a9a",
   "metadata": {},
   "source": [
    "If you are a [ZenML Cloud](https://zenml.io/cloud) user, you can see all of this visualized in the dashboard:\n",
    "\n",
    "<img src=\"_assets/cloud_mcp_screenshot.png\" width=\"70%\" alt=\"Model Control Plane\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb645dde",
   "metadata": {},
   "source": [
    "There is a lot more you can do with ZenML models, including the ability to\n",
    "track metrics by adding metadata to it, or having them persist in a model\n",
    "registry. However, these topics can be explored more in the\n",
    "[ZenML docs](https://docs.zenml.io).\n",
    "\n",
    "For now, we will use the ZenML model control plane to promote our best\n",
    "model to `production`. You can do this by simply setting the `stage` of\n",
    "your chosen model version to the `production` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b718f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our best classifier to production\n",
    "xgboost_zenml_model_version.set_stage(\"production\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddf3d0",
   "metadata": {},
   "source": [
    "Of course, normally one would only promote the model by comparing to all other model\n",
    "versions and doing some other tests. But that's a bit more advanced use-case. See the\n",
    "[e2e_batch example](https://github.com/zenml-io/zenml/tree/main/examples/e2e) to get\n",
    "more insight into that sort of flow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecbc8cf",
   "metadata": {},
   "source": [
    "<img src=\"_assets/cloud_mcp.png\" width=\"60%\" alt=\"Model Control Plane\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1146db",
   "metadata": {},
   "source": [
    "Once the model is promoted, we can now consume the right model version in our\n",
    "batch inference pipeline directly. Let's see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6306f14",
   "metadata": {},
   "source": [
    "# ü´Ö Step 2: Consuming the model in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f3108",
   "metadata": {},
   "source": [
    "The batch inference pipeline simply takes the model marked as `production` and runs inference on it\n",
    "with `live data`. The critical step here is the `inference_predict` step, where we load the model in memory\n",
    "and generate predictions:\n",
    "\n",
    "<img src=\"_assets/inference_pipeline.png\" width=\"45%\" alt=\"Inference pipeline\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c4c7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m# Apache Software License 2.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Copyright (c) ZenML GmbH 2023. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# you may not use this file except in compliance with the License.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# You may obtain a copy of the License at\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# http://www.apache.org/licenses/LICENSE-2.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Unless required by applicable law or agreed to in writing, software\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0minference_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_inf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Predictions step.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This is an example of a predictions step that takes the data and model in\u001b[0m\n",
      "\u001b[0;34m    and returns predicted values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This step is parameterized, which allows you to configure the step\u001b[0m\n",
      "\u001b[0;34m    independently of the step code, before running it in a pipeline.\u001b[0m\n",
      "\u001b[0;34m    In this example, the step can be configured to use different input data.\u001b[0m\n",
      "\u001b[0;34m    See the documentation for more information:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        https://docs.zenml.io/user-guide/advanced-guide/configure-steps-pipelines\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        model: Trained model.\u001b[0m\n",
      "\u001b[0;34m        dataset_inf: The inference dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m        The predictions as pandas series\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# run prediction from memory\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_inf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"predicted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at training pipeline\n",
    "%pycat steps/inference_predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb227b",
   "metadata": {},
   "source": [
    "Apart from the loading the model, we must also load the preprocessing pipeline that we ran in feature engineering,\n",
    "so that we can do the exact steps that we did on training time, in inference time. Let's bring it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c409bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m# Apache Software License 2.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Copyright (c) ZenML GmbH 2024. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# you may not use this file except in compliance with the License.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# You may obtain a copy of the License at\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# http://www.apache.org/licenses/LICENSE-2.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# Unless required by applicable law or agreed to in writing, software\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minference_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minference_preprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_pipeline_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Model inference pipeline.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This is a pipeline that loads the inference data, processes it with\u001b[0m\n",
      "\u001b[0;34m    the same preprocessing pipeline used in training, and runs inference\u001b[0m\n",
      "\u001b[0;34m    with the trained model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        random_state: Random state for reproducibility.\u001b[0m\n",
      "\u001b[0;34m        target: Name of target column in dataset.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Get the production model artifact\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pipeline_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"breast_cancer_classifier\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Get the preprocess pipeline artifact associated with this version\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpreprocess_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pipeline_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"preprocess_pipeline\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Link all the steps together by calling them and passing the output\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m#  of one step as the input of the next step.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdf_inference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdf_inference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_inf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_inference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpreprocess_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minference_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_inf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_inference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# let's have a look at training pipeline\n",
    "%pycat pipelines/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afe7be",
   "metadata": {},
   "source": [
    "The way to load the right model is to pass in the `production` stage into the `Model` config this time.\n",
    "This will ensure to always load the production model, decoupled from all other pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_settings = {\"enable_cache\": False}\n",
    "\n",
    "# Lets add some metadata to the model to make it identifiable\n",
    "pipeline_settings[\"model\"] = Model(\n",
    "    name=\"breast_cancer_classifier\",\n",
    "    version=\"production\", # We can pass in the stage name here!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3402f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `with_options` method allows us to pass in pipeline settings\n",
    "#  and returns a configured pipeline\n",
    "inference.with_options(**pipeline_settings)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2935d1fa",
   "metadata": {},
   "source": [
    "ZenML automatically links all artifacts to the `production` model version as well, including the predictions\n",
    "that were returned in the pipeline. This completes the MLOps loop of training to inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch production model\n",
    "production_model_version = client.get_model_version(\"breast_cancer_classifier\", \"production\")\n",
    "\n",
    "# Get the predictions artifact\n",
    "production_model_version.get_artifact(\"predictions\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a73cdf",
   "metadata": {},
   "source": [
    "You can also see all predictions ever created as a complete history in the dashboard:\n",
    "\n",
    "<img src=\"_assets/cloud_mcp_predictions.png\" width=\"70%\" alt=\"Model Control Plane\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf22981",
   "metadata": {},
   "source": [
    "# üêô Step 3: Analyzing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model_version = client.list_model_versions(\"breast_cancer_classifier\",tag=\"sgd\")[-1]\n",
    "xgboost_model_version = client.list_model_versions(\"breast_cancer_classifier\",tag=\"xgboost\")[-1]\n",
    "print(f\"SGD version is staged as `{sgd_model_version.stage}`\")\n",
    "print(f\"XGBoost version is staged as `{xgboost_model_version.stage}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d36b23",
   "metadata": {},
   "source": [
    "At first, let's pull some meta information collected during models evaluation stage. To recall we used this step as evaluator:\n",
    "```python\n",
    "@step\n",
    "def model_evaluator(\n",
    "    model: ClassifierMixin,\n",
    "    dataset_trn: pd.DataFrame,\n",
    "    dataset_tst: pd.DataFrame,\n",
    "    min_train_accuracy: float = 0.0,\n",
    "    min_test_accuracy: float = 0.0,\n",
    "    target: Optional[str] = \"target\",\n",
    ") -> float:\n",
    "    # Calculate the model accuracy on the train and test set\n",
    "    trn_acc = model.score(...)\n",
    "    tst_acc = model.score(...)\n",
    "\n",
    "    ...\n",
    "    \n",
    "    predictions = model.predict(dataset_tst.drop(columns=[target]))\n",
    "    metadata = {\n",
    "        \"train_accuracy\": float(trn_acc),\n",
    "        \"test_accuracy\": float(tst_acc),\n",
    "        \"confusion_matrix\": confusion_matrix(dataset_tst[target], predictions)\n",
    "        .ravel()\n",
    "        .tolist(),\n",
    "    }\n",
    "    log_model_metadata(metadata={\"wandb_url\": wandb.run.url})\n",
    "    log_artifact_metadata(\n",
    "        metadata=metadata,\n",
    "        artifact_name=\"breast_cancer_classifier\",\n",
    "    )\n",
    "\n",
    "    wandb.log({\"train_accuracy\": metadata[\"train_accuracy\"]})\n",
    "    wandb.log({\"test_accuracy\": metadata[\"test_accuracy\"]})\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"confusion_matrix\": wandb.sklearn.plot_confusion_matrix(\n",
    "                dataset_tst[target], predictions, [\"No Cancer\", \"Cancer\"]\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    return float(tst_acc)\n",
    "```\n",
    "First we pull Accuracy metrics out of both model version for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce698de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf_metadata = sgd_model_version.get_artifact(\"breast_cancer_classifier\").run_metadata\n",
    "xgboost_clf_metadata = xgboost_model_version.get_artifact(\"breast_cancer_classifier\").run_metadata\n",
    "print(f\"SGD{' (production)' if sgd_model_version.stage == 'production' else ''} metrics: train={sgd_clf_metadata['train_accuracy'].value*100:.2f}% test={sgd_clf_metadata['test_accuracy'].value*100:.2f}%\")\n",
    "print(f\"XGBoost{' (production)' if xgboost_model_version.stage == 'production' else ''} metrics: train={xgboost_clf_metadata['train_accuracy'].value*100:.2f}% test={xgboost_clf_metadata['test_accuracy'].value*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb5256",
   "metadata": {},
   "source": [
    "Now lets' plot collected Confusion Matrixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(metadata_pointer, tp: str,ax):\n",
    "    confusion_matrix = np.array(metadata_pointer[\"confusion_matrix\"].value, dtype=float).reshape((2,2))\n",
    "    confusion_matrix /= np.sum(confusion_matrix)\n",
    "    sns.heatmap(confusion_matrix, annot=True,fmt='.2%',cmap=\"coolwarm\",ax=ax)\n",
    "    ax.set_title(f\"{tp} confusion matrix\")\n",
    "    ax.set_ylabel(\"Ground Label\")\n",
    "    ax.set_xlabel(\"Predicted Label\")\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,4))\n",
    "plot_confusion_matrix(sgd_clf_metadata, \"SGD\",ax[0])\n",
    "plot_confusion_matrix(xgboost_clf_metadata, \"RF\",ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb63aee",
   "metadata": {},
   "source": [
    "So far we were able to collect all the information we tracked using Model Control Plane, but we also had Weights&Biases tracking enabled - let's dive into.\n",
    "\n",
    "Thanks to Model Control Plane metadata we establish a nice connection between those 2 entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7097d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SGD version: {sgd_model_version.run_metadata[\"wandb_url\"].value}')\n",
    "print(f'XGBoost version: {xgboost_model_version.run_metadata[\"wandb_url\"].value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de78e7b",
   "metadata": {},
   "source": [
    "With Model Control Plane we can also easily track lineage of artifacts and pipeline runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8891b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artifact_name, versions in sgd_model_version.data_artifacts.items():\n",
    "    if versions:\n",
    "        print(f\"Existing version of `{artifact_name}`:\")\n",
    "        for version_name, artifact_ in  versions.items():\n",
    "            print(version_name, artifact_.data_type.attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d386dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_name, run_ in sgd_model_version.pipeline_runs.items():\n",
    "    print(run_name, run_.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee4fc-f102-4b99-bdc3-2f1670c87679",
   "metadata": {},
   "source": [
    "# üôè Step 4: Moving to production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe22780",
   "metadata": {},
   "source": [
    "Let's move some heavy lifting to the Sagemaker. This can be achieved using Sagemaker orchestrator.\n",
    "\n",
    "<img src=\"_assets/sagemaker_stack.png\" width=\"60%\" alt=\"Sagemaker stack\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2774cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack set sagemaker-pipelines-wandb\n",
    "!zenml stack describe sagemaker-pipelines-wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e37542",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.activate_stack(\"sagemaker-pipelines-wandb\")\n",
    "\n",
    "pipeline_settings = {}\n",
    "\n",
    "pipeline_settings[\"model\"] = Model(\n",
    "    name=\"breast_cancer_classifier\",\n",
    "    license=\"Apache 2.0\",\n",
    "    description=\"A breast cancer classifier\",\n",
    "    tags = [\"breast_cancer\", \"classifier\", \"xgboost\"]\n",
    ")\n",
    "\n",
    "training.with_options(config_path=\"configs/training_xgboost.yaml\",**pipeline_settings)(\n",
    "    model_type=\"xgboost\",\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950f97e",
   "metadata": {},
   "source": [
    "# üßô‚Äç‚ôÇÔ∏è Step 5: Deploy Sagemaker endpoint\n",
    "\n",
    "After training and promoting production-ready model version to `production` we can proceed deploying it as a Sagemaker Endpoint.\n",
    "The deployment pipeline is following:\n",
    "\n",
    "<img src=\"_assets/deployment_pipeline.png\" width=\"60%\" alt=\"Deployment pipeline\">\n",
    "\n",
    "First we will explore how deployment step is designed. Based on the origin of current model in `production` we need to adapt deployment action a bit. For `XGBoost` we can use Sagemaker standard image and model saved via standard XGBoost `save_model` model, but for `sklearn` this approach will not work properly, since there is no standard way of saving Sklearn models besides pickling them via various libraries, so we need a special instruction script describing how to load, predict and so on on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496adffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_uris\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_step_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArtifactConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_artifact_metadata\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maws\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_aws_config\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_materializer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSagemakerPredictorMaterializer\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0menable_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_materializers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSagemakerPredictorMaterializer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mdeploy_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mPredictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mArtifactConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sagemaker_endpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_deployment_artifact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_aws_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;34m\"sgd\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mimage_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mregion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sklearn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1.0-1\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mentry_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utils/sklearn_inference.py\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mimage_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mregion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"xgboost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1.5-1\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mentry_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34mf'{model.get_artifact(\"breast_cancer_classifier\").uri}/model.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mendpoint_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'breast-cancer-classifier-{datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")}'\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mimage_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mentry_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ml.m5.large\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlog_artifact_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"endpoint_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"image_uri\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"role_arn\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pycat steps/deploy_endpoint.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60b684",
   "metadata": {},
   "source": [
    "The full deployment pipeline code is the following.\n",
    "\n",
    "NOTE: we deprovision model at the end of the pipeline to save cost, but this step will not be there in production setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dfb5642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mfrom\u001b[0m \u001b[0mzenml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_pipeline_context\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_preprocessor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeploy_endpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_on_endpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshutdown_endpoint\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshutdown_endpoint_after_predicting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Get the preprocess pipeline artifact associated with this version\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpreprocess_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pipeline_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"preprocess_pipeline\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdf_inference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdf_inference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset_inf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_inference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpreprocess_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeploy_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpredict_on_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_inference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mshutdown_endpoint_after_predicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mshutdown_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predict_on_endpoint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pycat pipelines/deploy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba94464",
   "metadata": {},
   "source": [
    "Ok, now we can deploy our model and explore predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines import deploy\n",
    "\n",
    "client.activate_stack(\"local-sagemaker-step-operator-wandb\")\n",
    "\n",
    "deploy.with_options(\n",
    "    model=Model(name=\"breast_cancer_classifier\", version=\"production\")\n",
    ")(shutdown_endpoint_after_predicting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cec4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore created endpoint\n",
    "run_metadata = client.get_model_version(\"breast_cancer_classifier\", \"production\").get_artifact(\"sagemaker_endpoint\").run_metadata\n",
    "for k,v in run_metadata.items():\n",
    "    print(k, v.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore real time predictions\n",
    "client.get_model_version(\"breast_cancer_classifier\", \"production\").get_artifact(\"real_time_predictions\").load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
