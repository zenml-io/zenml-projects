"""Correctness judging for agent answers using LLM evaluation."""

from textwrap import dedent

from environment.models import Scenario
from litellm import acompletion
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt


class CorrectnessJudgeResponse(BaseModel):
    """Response from the correctness judge LLM."""

    reasoning: str = Field(description="Explanation of the reasoning process.")
    accept: bool = Field(
        description="Whether the AI answer should be accepted."
    )


@retry(stop=stop_after_attempt(3))
async def judge_correctness(
    scenario: Scenario,
    answer: str,
    judge_model: str = "openai/gpt-4.1",
) -> CorrectnessJudgeResponse:
    """Judge whether an agent's answer is correct.

    Uses an LLM to compare the agent's answer against the reference answer,
    checking for semantic correctness rather than exact matching.

    Args:
        scenario: The scenario containing the question and reference answer.
        answer: The agent's generated answer.
        judge_model: The LiteLLM model identifier for the judge.

    Returns:
        CorrectnessJudgeResponse with reasoning and accept/reject decision.
    """
    system_prompt = dedent(
        """
        You are given a question, the reference answer (labelled **Reference
        answer**), and an answer generated by an AI assistant (labelled
        **AI answer**).

        Your task is to decide whether the AI answer is correct and should be
        accepted. You should accept the answer if it contains the relevant
        information from the reference answer. You should not accept the answer
        if it is missing information relevant to the question, or if it
        contradicts the reference answer.
        """
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": (
                f"Question: {scenario.question}\n"
                f"Reference answer: {scenario.answer}\n"
                f"AI answer: {answer}"
            ),
        },
    ]

    response = await acompletion(
        model=judge_model,
        messages=messages,
        response_format=CorrectnessJudgeResponse,
    )

    first_choice = response.choices[0]
    raw_content = first_choice.message.content or "{}"

    try:
        return CorrectnessJudgeResponse.model_validate_json(raw_content)
    except Exception as e:
        return CorrectnessJudgeResponse(
            reasoning=f"Parse error: {e}\nRaw: {raw_content}",
            accept=False,
        )
