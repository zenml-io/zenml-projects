# Apache Software License 2.0
# 
# Copyright (c) ZenML GmbH 2024. All rights reserved.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# 

model:
  name: llm-peft-phi-3.5-mini-instruct-cpu
  description: "Fine-tune Phi-3.5-mini-instruct on CPU."
  tags:
    - llm
    - peft
    - phi-3.5
    - cpu
  version: 100_steps

settings:
  docker:
    parent_image: pytorch/pytorch:2.2.2-runtime
    requirements: requirements.txt
    python_package_installer: uv
    python_package_installer_args:
      system: null
    apt_packages: 
      - git
    environment:
      MKL_SERVICE_FORCE_INTEL: "1"
      # Explicitly disable MPS
      PYTORCH_ENABLE_MPS_FALLBACK: "0"
      PYTORCH_MPS_HIGH_WATERMARK_RATIO: "0.0"

parameters:
  # Uses a smaller model for CPU training
  base_model_id: microsoft/Phi-3.5-mini-instruct
  use_fast: False
  load_in_4bit: False
  load_in_8bit: False
  cpu_only: True  # Enable CPU-only mode
  # Extra conservative dataset size for CPU
  max_train_samples: 50
  max_val_samples: 10
  max_test_samples: 5
  system_prompt: |
      Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.
      This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].
      The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']
      

steps:
  prepare_data:
    parameters:
      dataset_name: gem/viggo
      # These settings are now defined at the pipeline level
      # max_train_samples: 100
      # max_val_samples: 20
      # max_test_samples: 10

  finetune:
    parameters:
      max_steps: 25  # Further reduced steps for CPU training
      eval_steps: 5  # More frequent evaluation
      bf16: False  # Disable bf16 for CPU compatibility
      per_device_train_batch_size: 1  # Smallest batch size for CPU
      gradient_accumulation_steps: 2  # Reduced for CPU
      optimizer: "adamw_torch"  # Use standard AdamW rather than 8-bit for CPU
      logging_steps: 2  # More frequent logging
      save_steps: 25  # Save less frequently 
      save_total_limit: 1  # Keep only the best model
      evaluation_strategy: "steps"

  promote:
    parameters:
      metric: rouge2
      target_stage: staging 