# Apache Software License 2.0
#
# Copyright (c) ZenML GmbH 2024. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

execution_mode: local

datasets:
  composite: "data/composite_dataset.jsonl"
  unclassified: "data/unclassified_dataset.jsonl"
  augmented: "data/augmented_dataset.jsonl"

outputs:
  classifications: "classification_results"
  ft_model: "models/finetuned_modernbert"
  ft_tokenizer: "models/finetuned_modernbert_tokenizer"
  model_compare_metrics: "model_compare_metrics"

model_repo_ids:
  deepseek: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
  modernbert_base_model: "answerdotai/ModernBERT-base"
  huggingface_repo: "your-username/your-repo-name"

project:
  name: ft_modernbert_classifier
  description: "Fine-tune `answerdotai/ModernBERT-base` for article classification."
  tags:
    - llmops
    - modernbert
    - article_classifier
  version: local

steps:
  classify:
    classification_type: "evaluation"

    batch_processing: # (Optional) - set to None to process entire dataset
      batch_start: 0
      batch_size: 40

    parallel_processing:
      enabled: true
      workers: 4

    checkpoint:
      enabled: true
      frequency: 10 # save checkpoint every N articles
      run_id: "batch-0-40" # set to a unique identifier to resume from a specific run

    inference_params:
      max_new_tokens: 1000
      max_sequence_length: 16384
      temperature: 0.2
      top_p: 0.3
      top_k: 30

  data_split:
    test_size: 0.2
    validation_size: 0.5

  finetune_modernbert:
    parameters:
      learning_rate: 3e-5
      lr_scheduler_type: "cosine"
      per_device_train_batch_size: 8
      per_device_eval_batch_size: 64
      num_train_epochs: 8
      warmup_ratio: 0.1
      label_smoothing_factor: 0.1
      weight_decay: 0.01
      max_grad_norm: 1.0
      load_best_model_at_end: true
      metric_for_best_model: "f1"
      eval_strategy: "epoch"
      save_strategy: "epoch"
      output_dir: "eval_results"
      logging_dir: "./logs"
      logging_strategy: "epoch"

  compare:
    dataset:
      source_type: "artifact" # or "disk"
      path: "artifacts/test_set" # used if source_type is "disk"
      artifact_name: "test_set"
      version: # (optional) set to specific version if source_type is "artifact"
    batch_sizes:
      modernbert: 25
      claude: 10
    costs:
      claude_haiku:
        input_cost_per_1k: 0.00025
        output_cost_per_1k: 0.000625
