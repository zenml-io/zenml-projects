# ğŸ§ª QualityFlow: AI-Powered Test Generation Pipeline

A streamlined MLOps pipeline for **automated test generation** using ZenML and LLMs. Generate comprehensive unit tests for your codebase, compare different approaches, and get detailed coverage analysis.

## ğŸš€ Product Overview

QualityFlow demonstrates how to build production-ready MLOps workflows for automated test generation using Large Language Models. Built with ZenML, it provides a simple yet powerful pipeline for generating and evaluating AI-generated tests.

**Focus**: **LLM-Powered Test Generation** and **Coverage Analysis**.

### Key Features

- **Real LLM Integration**: OpenAI and Anthropic providers for intelligent test generation
- **Smart File Selection**: Configurable strategies to focus on files that need testing
- **Baseline Comparison**: Compare LLM-generated tests vs heuristic baseline tests
- **Coverage Analysis**: Real coverage metrics with detailed reporting
- **Speed Controls**: `max_files` parameters to control pipeline execution time
- **Containerized Ready**: Uses ZenML Path artifacts for remote execution
- **Cost Tracking**: Token usage and cost estimation with metadata logging

## ğŸ’¡ How It Works

### âœˆï¸ Pipeline Architecture

QualityFlow consists of a single, focused pipeline:

#### Generate & Evaluate Pipeline

The main pipeline handles the complete test generation workflow:

1. **Source Selection** - Specify repository and target files
2. **Code Fetching** - Clone and materialize workspace 
3. **Code Analysis** - Select files for testing (with max_files limit)
4. **LLM Test Generation** - Generate tests using OpenAI/Anthropic/fake providers
5. **Baseline Generation** - Create simple heuristic tests for comparison
6. **Test Execution** - Run both test suites with coverage analysis
7. **Report Generation** - Compare results and generate markdown reports

### ğŸ”§ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Git Repo      â”‚    â”‚  LLM Providers  â”‚    â”‚   Test Reports  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  src/**/*.py    â”‚â”€â”€â”€â”€â”‚â–¶ OpenAI/Claude  â”‚â”€â”€â”€â”€â”‚â–¶ Coverage       â”‚
â”‚  target files   â”‚    â”‚  Fake (testing) â”‚    â”‚  Comparisons    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚  Cost Tracking  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                             â–²
         â–¼                                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   QualityFlow Pipeline                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                Generate & Evaluate                      â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚ 1. Select Input    â†’ 2. Fetch Source    â†’ 3. Analyze   â”‚   â”‚
â”‚  â”‚ 4. Generate (LLM)  â†’ 5. Generate (Base) â†’ 6. Run Tests â”‚   â”‚
â”‚  â”‚ 7. Run Tests       â†’ 8. Report & Compare               â”‚   â”‚
â”‚  â”‚                                                         â”‚   â”‚
â”‚  â”‚ Features: max_files control, Path artifacts, metadata  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Quick Start

### Prerequisites

- Python 3.9+
- ZenML installed (`pip install zenml`)
- Git
- OpenAI API key (optional, can use fake provider)

### Setup

```bash
pip install -r requirements.txt
```

2. **Set up OpenAI (optional)**:
```bash
export OPENAI_API_KEY="your-api-key-here"
```

3. **Run the pipeline**:
```bash
python run.py
```

That's it! The pipeline will:
- Clone the configured repository (default: requests library)
- Analyze Python files and select candidates
- Generate tests using OpenAI (or fake provider if no API key)
- Run tests and measure coverage
- Generate a comprehensive report comparing approaches

## âš™ï¸ Configuration

### Key Parameters

You can customize the pipeline behavior by editing `configs/experiment.default.yaml`:

```yaml
# Control execution speed
steps:
  analyze_code:
    parameters:
      max_files: 3  # Limit files to analyze (faster execution)
  
  gen_tests_agent:
    parameters:
      provider: "openai"  # openai | anthropic | fake
      model: "gpt-4o-mini"
      max_files: 2        # Limit files for test generation
      max_tests_per_file: 3

  gen_tests_baseline:
    parameters:
      max_files: 2        # Match agent for fair comparison
```

### Pipeline Options

```bash
# Use fake provider (no API key needed)
python run.py  # Uses config defaults

# Force fresh execution (no caching) 
python run.py --no-cache

# Use different config
python run.py --config configs/experiment.strict.yaml
```

## ğŸ”¬ Advanced Usage

### Different Target Repositories

Edit the config to point to your own repository:

```yaml
steps:
  select_input:
    parameters:
      repo_url: "https://github.com/your-org/your-repo.git"
      ref: "main"
      target_glob: "src/**/*.py"  # Adjust path pattern
```

### Custom Prompts

Create new Jinja2 templates in `prompts/`:

```jinja2
# prompts/custom_test_v3.jinja

Generate {{ max_tests }} tests for:
{{ file_path }} (complexity: {{ complexity_score }})

Source:
```python
{{ source_code }}
```

Requirements:
- Use pytest fixtures
- Include edge cases
- Mock external dependencies
```

### A/B Testing Experiments

Use run templates for systematic comparisons:

```bash
# Compare prompt versions
python scripts/run_experiment.py --config configs/experiment.default.yaml
python scripts/run_experiment.py --config configs/experiment.strict.yaml

# Compare in ZenML dashboard:
# - Coverage metrics
# - Test quality scores  
# - Token usage and cost
# - Promotion decisions
```

### Production Deployment

Set up ZenML stack for cloud deployment:

```bash
# Example: AWS EKS stack
zenml artifact-store register s3_store --flavor=s3 --path=s3://your-bucket
zenml container-registry register ecr_registry --flavor=aws --uri=your-account.dkr.ecr.region.amazonaws.com
zenml orchestrator register k8s_orchestrator --flavor=kubernetes --kubernetes_context=your-eks-context

zenml stack register production_stack \
  -a s3_store -c ecr_registry -o k8s_orchestrator --set
```

### Scheduled Regression

Register batch regression for daily execution:

```bash
python scripts/run_batch.py --config configs/schedule.batch.yaml --schedule
```

## ğŸ—ï¸ Project Structure

```
qualityflow/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ zenml.yaml
â”‚
â”œâ”€â”€ configs/                          # Pipeline configurations
â”‚   â”œâ”€â”€ experiment.default.yaml       # Standard experiment settings
â”‚   â”œâ”€â”€ experiment.strict.yaml        # High-quality gates
â”‚   â””â”€â”€ schedule.batch.yaml           # Batch regression schedule
â”‚
â”œâ”€â”€ domain/                           # Core data models
â”‚   â”œâ”€â”€ schema.py                     # Pydantic models
â”‚   â””â”€â”€ stages.py                     # Deployment stages
â”‚
â”œâ”€â”€ pipelines/                        # Pipeline definitions
â”‚   â”œâ”€â”€ generate_and_evaluate.py      # Experiment pipeline
â”‚   â””â”€â”€ batch_regression.py           # Scheduled regression
â”‚
â”œâ”€â”€ steps/                            # Pipeline steps
â”‚   â”œâ”€â”€ select_input.py               # Source specification
â”‚   â”œâ”€â”€ fetch_source.py               # Repository fetching  
â”‚   â”œâ”€â”€ analyze_code.py               # Code analysis & selection
â”‚   â”œâ”€â”€ gen_tests_agent.py            # LLM test generation
â”‚   â”œâ”€â”€ gen_tests_baseline.py         # Heuristic test generation
â”‚   â”œâ”€â”€ run_tests.py                  # Test execution & coverage
â”‚   â”œâ”€â”€ evaluate_coverage.py          # Metrics & gate evaluation
â”‚   â”œâ”€â”€ compare_and_promote.py        # Model registry promotion
â”‚   â”œâ”€â”€ resolve_test_pack.py          # Test pack resolution
â”‚   â””â”€â”€ report.py                     # Report generation
â”‚
â”œâ”€â”€ prompts/                          # Jinja2 prompt templates
â”‚   â”œâ”€â”€ unit_test_v1.jinja           # Standard test generation
â”‚   â””â”€â”€ unit_test_strict_v2.jinja    # Comprehensive test generation
â”‚
â”œâ”€â”€ materializers/                    # Custom artifact handling
â”œâ”€â”€ utils/                           # Utility functions
â”‚
â”œâ”€â”€ registry/                        # Test Pack registry docs
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ run_templates/                   # Experiment templates
â”‚   â”œâ”€â”€ ab_agent_vs_strict.json    # A/B testing configuration
â”‚   â””â”€â”€ baseline_only.json         # Baseline establishment
â”‚
â”œâ”€â”€ scripts/                        # CLI scripts
â”‚   â”œâ”€â”€ run_experiment.py          # Experiment runner
â”‚   â””â”€â”€ run_batch.py              # Batch regression runner
â”‚
â””â”€â”€ examples/                       # Demo code for testing
    â””â”€â”€ toy_lib/                   # Sample library
        â”œâ”€â”€ calculator.py
        â””â”€â”€ string_utils.py
```

### Key Components

- **Domain Models**: Pydantic schemas for type safety and validation
- **Pipeline Steps**: Modular, reusable components with clear interfaces
- **Prompt Templates**: Jinja2 templates for LLM test generation  
- **Configuration**: YAML-driven experiment and deployment settings
- **Quality Gates**: Configurable thresholds for coverage and promotion
- **Model Registry**: ZenML Model Registry integration for test pack versioning

## ğŸš€ Production Deployment

### ZenML Cloud Stack Setup

For production deployment with ZenML Cloud:

```bash
# Connect to ZenML Cloud
zenml connect --url https://your-org.zenml.cloud

# Register cloud stack components
zenml artifact-store register cloud_store --flavor=s3 --path=s3://qualityflow-artifacts
zenml orchestrator register cloud_k8s --flavor=kubernetes --kubernetes_context=prod-cluster

zenml stack register production \
  -a cloud_store -o cloud_k8s --set
```

### Scheduled Execution

Set up automated regression testing:

```bash
# Register schedule (example with ZenML Cloud)
python scripts/run_batch.py --config configs/schedule.batch.yaml --schedule

# Monitor via dashboard:
# - Daily regression results
# - Coverage trend analysis  
# - Test pack performance
```

## ğŸ¤ Contributing

QualityFlow follows ZenML best practices and is designed to be extended:

1. **Add New LLM Providers**: Extend `gen_tests_agent.py` with new provider integrations
2. **Custom Materializers**: Create materializers for new artifact types
3. **Additional Metrics**: Expand evaluation capabilities with new quality metrics
4. **Selection Strategies**: Add new code selection algorithms

## ğŸ“ Next Steps

After running QualityFlow successfully:

1. **Explore ZenML Dashboard**: View pipeline runs, artifacts, and model registry
2. **Experiment with Prompts**: Try different test generation strategies
3. **Add Real Codebases**: Replace toy examples with your production code
4. **Deploy to Production**: Use cloud orchestration for scale
5. **Set Up Monitoring**: Configure alerts for regression detection

## ğŸ†˜ Troubleshooting

### Common Issues

**LLM API Errors**:
- Set `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` environment variables
- Use `provider: "fake"` for development without API keys

**Test Execution Failures**:
- Ensure pytest and coverage tools are installed
- Check that workspace has proper Python path setup

### Debug Mode

Run with debug logging:

```bash
export ZENML_LOGGING_VERBOSITY=DEBUG
python scripts/run_experiment.py --config configs/experiment.default.yaml
```

## ğŸ“š Resources

- [ZenML Documentation](https://docs.zenml.io/)
- [Model Control Plane](https://docs.zenml.io/user-guide/model-control-plane)
- [Kubernetes Orchestrator](https://docs.zenml.io/stacks/stack-components/orchestrators/kubernetes)

---

Built with â¤ï¸ using [ZenML](https://zenml.io) - *The MLOps Framework for Production AI*