enable_cache: true

parameters:
  models:
    - "deepseek/deepseek-chat"     # DeepSeek V3 (placeholder for V4)
    - "anthropic/claude-sonnet-4-5-20250514"  # Claude as baseline
  judge_model: "anthropic/claude-sonnet-4-5-20250514"
  report_title: "LLM Code Evaluation Results"

steps:
  load_test_cases:
    parameters:
      subset_path: "test_cases/humaneval_subset.json"
      hard_problems_path: "test_cases/hard_problems.json"
      sample_size: 15
      seed: 42
      min_hard: 3

  run_inference:
    parameters:
      temperature: 0.0
      max_tokens: 1024
      timeout_s: 120

  evaluate_outputs:
    parameters:
      temperature: 0.0
      max_tokens: 512

  generate_report:
    parameters:
      include_per_problem_breakdown: true

settings:
  docker:
    requirements: requirements.txt
    python_package_installer: "uv"
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY}
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY}
      LANGFUSE_HOST: ${LANGFUSE_HOST}
