{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49ef75127f24058a606dcd8891fb040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268a1d39eb40453392f54d6dfeef112d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "hf_dataset = load_dataset(\"datadrivenscience/ship-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'objects'],\n",
       "        num_rows: 256\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'objects'],\n",
       "        num_rows: 385\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Process train and test splits\u001b[39;00m\n\u001b[1;32m     36\u001b[0m train_data \u001b[38;5;241m=\u001b[39m process_split(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Save the data as JSON files\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mprocess_split\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_split\u001b[39m(split):\n\u001b[1;32m     16\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hf_dataset[split]):\n\u001b[1;32m     18\u001b[0m         image \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m         objects \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/datasets/arrow_dataset.py:2440\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2438\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m   2439\u001b[0m             pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2440\u001b[0m             formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2441\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpa_subtable_ex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2443\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2444\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2445\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_all_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2446\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2447\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m formatted_output\n\u001b[1;32m   2448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/datasets/formatting/formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/datasets/formatting/formatting.py:397\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/datasets/formatting/formatting.py:438\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    437\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_row(pa_table)\n\u001b[0;32m--> 438\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/datasets/formatting/formatting.py:216\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_row\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m row\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/datasets/features/features.py:1975\u001b[0m, in \u001b[0;36mFeatures.decode_example\u001b[0;34m(self, example, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \n\u001b[1;32m   1964\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[38;5;124;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[1;32m   1973\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m   1976\u001b[0m         column_name: decode_nested_example(feature, value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[1;32m   1977\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1978\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[1;32m   1979\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[1;32m   1980\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[1;32m   1981\u001b[0m         )\n\u001b[1;32m   1982\u001b[0m     }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/datasets/features/features.py:1976\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \n\u001b[1;32m   1964\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[38;5;124;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[1;32m   1973\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m-> 1976\u001b[0m         column_name: \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1977\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1978\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[1;32m   1979\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[1;32m   1980\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[1;32m   1981\u001b[0m         )\n\u001b[1;32m   1982\u001b[0m     }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/datasets/features/features.py:1341\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[0;32m-> 1341\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/datasets/features/image.py:188\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(bytes_))\n\u001b[0;32m--> 188\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mgetexif()\u001b[38;5;241m.\u001b[39mget(PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mExifTags\u001b[38;5;241m.\u001b[39mBase\u001b[38;5;241m.\u001b[39mOrientation) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImageOps\u001b[38;5;241m.\u001b[39mexif_transpose(image)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/ls51/lib/python3.10/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Specify the base directory for storing the data\n",
    "base_dir = \"/home/strickvl/coding/zenml-projects/end-to-end-computer-vision/data\"\n",
    "\n",
    "# Create directories to store the extracted images\n",
    "os.makedirs(os.path.join(base_dir, \"train_images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, \"test_images\"), exist_ok=True)\n",
    "\n",
    "# Function to process each split (train and test)\n",
    "def process_split(split):\n",
    "    data = []\n",
    "    for idx, example in enumerate(hf_dataset[split]):\n",
    "        image = example[\"image\"]\n",
    "        objects = example[\"objects\"]\n",
    "        \n",
    "        # Save the image to the local filesystem\n",
    "        image_filename = f\"{split}_{idx}.jpg\"\n",
    "        image_path = os.path.join(base_dir, f\"{split}_images\", image_filename)\n",
    "        with open(image_path, \"wb\") as f:\n",
    "            f.write(image.tobytes())\n",
    "        \n",
    "        # Create a dictionary with image filename and bounding boxes\n",
    "        data.append({\n",
    "            \"image_filename\": image_filename,\n",
    "            \"bboxes\": objects[\"bbox\"]\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Process train and test splits\n",
    "train_data = process_split(\"train\")\n",
    "test_data = process_split(\"test\")\n",
    "\n",
    "# Save the data as JSON files\n",
    "with open(os.path.join(base_dir, \"train_data.json\"), \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(os.path.join(base_dir, \"test_data.json\"), \"w\") as f:\n",
    "    json.dump(test_data, f)\n",
    "\n",
    "# Save the data as Parquet files\n",
    "train_table = pa.Table.from_pylist(train_data)\n",
    "pq.write_table(train_table, os.path.join(base_dir, \"train_data.parquet\"))\n",
    "\n",
    "test_table = pa.Table.from_pylist(test_data)\n",
    "pq.write_table(test_table, os.path.join(base_dir, \"test_data.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories saved as categories.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create a dictionary to store the categories for each split\n",
    "categories_dict = {\n",
    "    \"train\": [example[\"objects\"][\"categories\"] for example in hf_dataset[\"train\"]],\n",
    "    \"test\": [example[\"objects\"][\"categories\"] for example in hf_dataset[\"test\"]]\n",
    "}\n",
    "\n",
    "# Save the categories as a JSON file\n",
    "with open(\"categories.json\", \"w\") as json_file:\n",
    "    json.dump(categories_dict, json_file)\n",
    "\n",
    "print(\"Categories saved as categories.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features: {'image': Image(mode=None, decode=True, id=None), 'objects': {'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), 'categories': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}}\n",
      "Test features: {'image': Image(mode=None, decode=True, id=None), 'objects': {'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), 'categories': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}}\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset features\n",
    "print(\"Train features:\", hf_dataset[\"train\"].features)\n",
    "print(\"Test features:\", hf_dataset[\"test\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images for 'test' extracted and resized successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    # Create a directory to store the extracted images\n",
    "    output_dir = f\"/home/strickvl/coding/zenml-projects/end-to-end-computer-vision/data/ships/{split}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Specify the maximum size for the resized images\n",
    "    max_size = (1000, 1000)\n",
    "\n",
    "    # Increase the maximum image size limit\n",
    "    Image.MAX_IMAGE_PIXELS = 1_000_000_000  # Adjust the limit as needed\n",
    "\n",
    "    # Iterate over the examples in the specified split\n",
    "    for i, example in enumerate(hf_dataset[split]):\n",
    "        # Get the image data from the example\n",
    "        image_data = example[\"image\"]\n",
    "        \n",
    "        # Resize the image while maintaining the aspect ratio\n",
    "        image_data.thumbnail(max_size)\n",
    "        \n",
    "        # Generate a unique filename for the image\n",
    "        image_filename = f\"image_{i}.png\"\n",
    "        \n",
    "        # Save the resized image data to the output directory\n",
    "        image_path = os.path.join(output_dir, image_filename)\n",
    "        image_data.save(image_path)\n",
    "\n",
    "    print(f\"Images for '{split}' extracted and resized successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||----------------|   1/256 [5.2ms elapsed, 1.3s remaining, 191.7 samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 256/256 [24.8ms elapsed, 0s remaining, 10.3K samples/s]  \n",
      "Name:        ships\n",
      "Media type:  image\n",
      "Num samples: 256\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:       fiftyone.core.fields.ObjectIdField\n",
      "    filepath: fiftyone.core.fields.StringField\n",
      "    tags:     fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "[<Sample: {\n",
      "    'id': '6627738801d54b9ea7b3c377',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/home/strickvl/coding/zenml-projects/end-to-end-computer-vision/data/ships/image_0.png',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "}>, <Sample: {\n",
      "    'id': '6627738801d54b9ea7b3c378',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/home/strickvl/coding/zenml-projects/end-to-end-computer-vision/data/ships/image_1.png',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "}>, <Sample: {\n",
      "    'id': '6627738801d54b9ea7b3c379',\n",
      "    'media_type': 'image',\n",
      "    'filepath': '/home/strickvl/coding/zenml-projects/end-to-end-computer-vision/data/ships/image_10.png',\n",
      "    'tags': [],\n",
      "    'metadata': None,\n",
      "}>]\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "name = \"ships\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=output_dir,\n",
    "    dataset_type=fo.types.ImageDirectory,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "# View summary info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make initial predictions using pre-trained Yolov8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Ultralytics logging\n",
    "import os; os.environ[\"YOLO_VERBOSE\"] = \"False\"\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.utils.ultralytics as fou\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv8\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "\n",
    "dataset.apply_model(model, label_field=\"boxes\")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The backend 'labelstudio' does not support attributes. Provided attributes will be ignored.\n",
      "Uploading media to Label Studio...\n",
      "Upload complete\n",
      "Launching editor at 'http://localhost:8080/projects/8'...\n",
      "{\n",
      "    \"key\": \"shipdet1\",\n",
      "    \"version\": \"0.23.8\",\n",
      "    \"timestamp\": \"2024-04-23T08:51:56.926945\",\n",
      "    \"config\": {\n",
      "        \"cls\": \"fiftyone.utils.labelstudio.LabelStudioBackendConfig\",\n",
      "        \"type\": \"annotation\",\n",
      "        \"method\": \"labelstudio\",\n",
      "        \"name\": \"labelstudio\",\n",
      "        \"label_schema\": {\n",
      "            \"new_ground_truth\": {\n",
      "                \"type\": \"detections\",\n",
      "                \"classes\": [\n",
      "                    \"ship\"\n",
      "                ],\n",
      "                \"attributes\": {},\n",
      "                \"existing_field\": false\n",
      "            }\n",
      "        },\n",
      "        \"media_field\": \"filepath\",\n",
      "        \"url\": \"http://localhost:8080\",\n",
      "        \"project_name\": null\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Send samples to Label Studio\n",
    "\n",
    "# A unique identifier for this run\n",
    "anno_key = \"shipdet1\"\n",
    "\n",
    "label_schema = {\n",
    "    \"new_ground_truth\": {\n",
    "        \"type\": \"detections\",\n",
    "        \"classes\": [\"ship\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "dataset.annotate(\n",
    "    anno_key,\n",
    "    backend=\"labelstudio\",\n",
    "    label_schema=label_schema,\n",
    "    launch_editor=True,\n",
    "    url=\"http://localhost:8080\",\n",
    "    api_key=\"9896fa7abd15277ab9046842469fe3c2ec17e0bb\"\n",
    ")\n",
    "print(dataset.get_annotation_info(anno_key))\n",
    "\n",
    "# Step 4: Perform annotation in Label Studio and save the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading labels from Label Studio...\n",
      "Download complete\n",
      "Loading labels for field 'new_ground_truth'...\n",
      " 100% |███████████████████| 15/15 [12.7ms elapsed, 0s remaining, 1.2K samples/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=1bb98b2d-af0a-4154-9edf-46a9080e7c75\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7aa725c06170>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# Step 5: Merge annotations back into FiftyOne dataset\n",
    "\n",
    "dataset = fo.load_dataset(name)\n",
    "dataset.load_annotations(anno_key)\n",
    "\n",
    "# Load the view that was annotated in the App\n",
    "view = dataset.load_annotation_view(anno_key)\n",
    "session = fo.launch_app(view=view)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=ec9d51cf-631a-4e7b-a4f4-0a20cb7c043e\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7192c399ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset:     -\n",
       "Session URL: http://localhost:5151/"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "fo.launch_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls51",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
