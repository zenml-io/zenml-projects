{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e6faed",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"attachment:_assets/Logo/zenml.svg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefc612",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"_assets/Logo/zenml.svg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from absl import logging as absl_logging\n",
    "absl_logging.set_verbosity(-10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eaaa90",
   "metadata": {},
   "source": [
    "# Why ZenML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fde8c7",
   "metadata": {},
   "source": [
    "![Sam](_assets/sam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d37b78",
   "metadata": {},
   "source": [
    "Let's get into this. \n",
    "But first things first. We need to initialize our zenml repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ddf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml init\n",
    "# Create a local stack to run these pipelines\n",
    "!zenml stack register local_stack -a default -o default\n",
    "!zenml stack set local_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ba7bc",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33912e23",
   "metadata": {},
   "source": [
    "A couple weeks ago, we were looking for a fun project to work on for the next chapter of our ZenHacks. During our initial discussions, we realized that it would be really great to work with an NBA dataset, as we could quickly get close to a real-life application like a \"3-Pointer Predictor\" while simultaneously entertaining ourselves with one of the trending topics within our team.\n",
    "\n",
    "As we were building the dataset around a \"3-Pointer Predictor\", we realized that there is one factor that we need to take into consideration first: Stephen Curry, The Baby Faced Assassin. In our opinion, there is no denying that he changed the way that the games are played in the NBA and we wanted to actually prove that this was the case first. \n",
    "\n",
    "That's why our story in this ZenHack will start with a pipeline dedicated to drift detection. As the breakpoint of this drift, we will be using the famous \"Double Bang\" game that the Golden State Warriors played against Oklahoma City Thunder back in 2016. Following that, we will build a training pipeline which will generate a model that predicts the number of three-pointers made by a team in a single game, and ultimately, we will use these trained models and create an inference pipeline for the upcoming matches in the NBA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c02d0",
   "metadata": {},
   "source": [
    "# Chapter 1 - Exploring NBA Data\n",
    "## Did Steph Curry Change the Game?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ddbed",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=GEMVGHoenXM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4108c6",
   "metadata": {},
   "source": [
    "![Steph Curry Drains the Game Winner vs Oklahoma City](https://i.makeagif.com/media/3-20-2016/7N5RWB.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52add8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this date in our pipelines as the division between old and new\n",
    "CURRYS_THREE_POINTER = '2016-02-27'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e2f66",
   "metadata": {},
   "source": [
    "![PipelineStructure](_assets/DriftDetectionPipeline.png \"PipelineStructure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27700042",
   "metadata": {},
   "source": [
    "## Creating our first step\n",
    "\n",
    "Naturally our first step should be the data import. For this we query the nba_api for all data for a set of seasons. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Best practice would be to disable cache for steps that fetch external data. ZenML has no way of knowing if this data has changed and would always cache the step if caching is not explicitly disabled.\n",
    "</div>\n",
    "\n",
    "For development it is useful to have caching enabled to enable faster iterative development of downstream steps.\n",
    "Use `@step(enable_cache=False)` to disable cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.steps import step\n",
    "from steps.importer import ImporterConfig\n",
    "import pandas as pd\n",
    "\n",
    "@step\n",
    "def game_data_importer(config: ImporterConfig) -> pd.DataFrame:\n",
    "    \"\"\"Downloads season data from NBA API and returns a pd.DataFrame\"\"\"\n",
    "    dataframes = []\n",
    "    for season in config.seasons:\n",
    "        print(f\"Fetching data for season: {season}\")\n",
    "        dataframes.append(leaguegamelog.LeagueGameLog(season=season, timeout=180).get_data_frames()[0])\n",
    "        # sleep so as not to bomb api server :-)\n",
    "        time.sleep(2)\n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627c7c5",
   "metadata": {},
   "source": [
    "## Creating an exploratory pipeline\n",
    "\n",
    "This is where we configure the steps of our pipeline and how data will flow from one step into the other. \n",
    "\n",
    "For this we use the `@pipeline decorator`. To define a pipeline we first define all steps of the pipeline in the function signature. Then within the function we configure how the outputs of steps get passed into steps downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225c6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from nba_api.stats.endpoints import leaguegamelog\n",
    "from zenml.pipelines import pipeline\n",
    "\n",
    "@pipeline\n",
    "def data_analysis_pipeline(\n",
    "        importer,          # Import NBA game data\n",
    "        drift_splitter,    # Split data at relevant date\n",
    "        drift_detector,    # Compare data distributions\n",
    "):\n",
    "    \"\"\"Links all the steps together in a pipeline\"\"\"\n",
    "    raw_data = importer()\n",
    "    reference_dataset, comparison_dataset = drift_splitter(raw_data)\n",
    "    drift_report, _ = drift_detector(reference_dataset, comparison_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b92917",
   "metadata": {},
   "source": [
    "## Integrating Evidently\n",
    "\n",
    "Evidently is an open source tool that allows you to easily compute drift on your data. [Here](https://blog.zenml.io/zenml-loves-evidently/) is a little blog post of ours that explains the evidently integration in a bit more detail. \n",
    "\n",
    "At its core, Evidently’s drift detection calculation functions take in a reference data set and compare it with a separate comparison dataset. These are both passed in as Pandas dataframes, though CSV inputs are also possible. ZenML implements this functionality in the form of several standardized steps along with an easy way to use the visualization tools also provided along with Evidently as ‘Dashboards’.\n",
    "\n",
    "\n",
    "If you’re working on any kind of machine learning problem that has an ongoing training loop that takes in new data, you’ll want to guard against drift. Machine learning pipelines are built on top of data inputs, so it is worth checking for drift if you have a model that was trained on a certain distribution of data. The incoming data is something you have less control over and since things often change out in the real world, you should have a plan for knowing when things have shifted. Evidently offers a [growing set of features](https://github.com/evidentlyai/evidently) that help you monitor not only data drift but other key aspects like target drift and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01338745",
   "metadata": {},
   "source": [
    "![Evidently](_assets/zenml+evidently.png \"Evidently\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa3006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to install evidently to our python environment\n",
    "!zenml integration install evidently -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we need to add evidently data validator to stack\n",
    "!zenml data-validator register local_evidently --flavor=evidently\n",
    "!zenml stack set local_stack\n",
    "!zenml stack update local_stack -dv local_evidently    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6d9af",
   "metadata": {},
   "source": [
    "Here we choose the [datadrift profile](https://docs.evidentlyai.com/get-started/reports/data-drift#data-drift-dashboard-examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce886fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zenml provides some standard steps for the evidently integration\n",
    "from zenml.integrations.evidently.steps import (\n",
    "    EvidentlyProfileConfig,\n",
    "    EvidentlyProfileStep,\n",
    ")\n",
    "\n",
    "# We create a config object for our evidently step - \n",
    "#  here we choose the datadrift profile \n",
    "evidently_drift_detector_config = EvidentlyProfileConfig(\n",
    "    column_mapping=None,profile_sections=[\"datadrift\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdb618",
   "metadata": {},
   "source": [
    "### Add step implementations to the pipeline and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d019fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from steps.splitter import date_based_splitter, SplitConfig\n",
    "\n",
    "# We also configure our data splitter - In this case we want to compare data before\n",
    "#  Steph Curry's infamous three-pointer to afterwards\n",
    "data_split_config = SplitConfig(date_split=CURRYS_THREE_POINTER, columns=['FG3M'])\n",
    "\n",
    "# Instantiate the pipeline\n",
    "#  For this we need to pass all our step implementations. \n",
    "#  At this stage the step configurations are passed to the correspondign steps\n",
    "eda_pipeline = data_analysis_pipeline(\n",
    "    importer=game_data_importer(),\n",
    "    drift_splitter=date_based_splitter(data_split_config),\n",
    "    drift_detector=EvidentlyProfileStep(evidently_drift_detector_config),\n",
    ")\n",
    "\n",
    "eda_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d931d2a",
   "metadata": {},
   "source": [
    "## Post-execution: Fetching pipelines and reviewing results\n",
    "\n",
    "Once our pipeline has run we now want to inspect and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd46b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.visualizers import EvidentlyVisualizer\n",
    "from zenml.post_execution import get_pipeline\n",
    "import json\n",
    "\n",
    "p = get_pipeline(pipeline_name='data_analysis_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our pipeline can have multiple runs associated with it\n",
    "p.runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac82274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this we want to look at the last run, the runs are sorted chronologically\n",
    "last_run = p.runs[-1]\n",
    "last_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_detection_step = last_run.get_step(\n",
    "    name=\"drift_detector\"\n",
    ")\n",
    "drift_detection_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76582b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "EvidentlyVisualizer().visualize(drift_detection_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a32c4",
   "metadata": {},
   "source": [
    "__Conclusion__: Ever since Steph Currys game in 2016, there has been a drift in how many three pointers are scored in the NBA. Is this all thanks to Curry? We won't claim any causation here. But we can say for sure the the amount of three pointers in the NBA has increased in the last few years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6eff38",
   "metadata": {},
   "source": [
    "# Chapter 2 - Training Pipeline \n",
    "\n",
    "Let's move on to our machine learning task. The diagram below is the result of an internal brainstorming session of what cool usecase we want to demonstrate. Here we have a continuous training pipeline that does two things. \n",
    "\n",
    "For one we import data from the NBA api and calculate if there hase been any significant drift in the amount of three pointers within the last week in comparison with all past data from 2016 onwards. \n",
    "\n",
    "On the other hand we also have a training pipeline that takes in the raw data from the NBA, does some basic feature engineering, encode the data and feeds it into the trainer/tester steps. The purpose of the trained model is to predict based on very little input data (two teams facing each other and the season id) how many three pointers the home team will score.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> The purpose of this notebook is <b>not</b> to train the best, most state-of-the-art model for the task. The purpose is to show you how to quickly set up a scalable, deployable and extensible machine learning pipeline that can go from ideation to production in no time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b947723",
   "metadata": {},
   "source": [
    "![Training Pipeline](_assets/TrainingPipeline.png \"Planned Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404e1f6",
   "metadata": {},
   "source": [
    "For this pipeline we want to take you a step further by showing you some more integrations. We will be using MLFlow Tracking for visualizing and comparing multiple pipeline runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338c10d",
   "metadata": {},
   "source": [
    "![Mlflow](_assets/zenml+evidently+mlflow.png \"Mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976de746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start off by installing the required packages\n",
    "!zenml integration install mlflow -f\n",
    "\n",
    "# Then we register an experiment tracker with mlflow flavor\n",
    "!zenml experiment-tracker register local_mlflow_tracker --flavor=mlflow\n",
    "!zenml stack set local_stack\n",
    "!zenml stack update local_stack -e local_mlflow_tracker    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3a117",
   "metadata": {},
   "source": [
    "After showing you a local pipeline run with mlflow tracking we will then continue on to changing our orchestrator to a kubeflow pipeline. \n",
    "\n",
    "As an additional little nugget we have also implemented a Discord step, which post into our company internal Discord channel whenever the drift is analyzed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d74c1",
   "metadata": {},
   "source": [
    "![All](_assets/evidently+mlflow+discord+kubeflow.png \"All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install kubeflow -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b74d19",
   "metadata": {},
   "source": [
    "### Build the pipeline definition\n",
    "\n",
    "Just like above we start off by defining the steps of our pipeline and the flow of inputs and outputs through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3206b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def training_pipeline(\n",
    "        importer,\n",
    "        feature_engineerer,\n",
    "        encoder,\n",
    "        ml_splitter,\n",
    "        trainer,\n",
    "        tester,\n",
    "        drift_splitter,\n",
    "        drift_detector,\n",
    "        drift_alert\n",
    "):\n",
    "    \"\"\"Links all the steps together in a pipeline\"\"\"\n",
    "    # Data Preprocessing\n",
    "    raw_data = importer()\n",
    "    transformed_data = feature_engineerer(raw_data)\n",
    "    encoded_data, le_seasons, ohe_teams = encoder(transformed_data)\n",
    "    train_df_x, train_df_y, test_df_x, test_df_y, eval_df_x, eval_df_y = ml_splitter(encoded_data)\n",
    "    \n",
    "    # Model training\n",
    "    model = trainer(train_df_x, train_df_y, eval_df_x, eval_df_y)\n",
    "    test_results = tester(model, test_df_x, test_df_y)\n",
    "\n",
    "    # drift detection branch\n",
    "    reference_dataset, comparison_dataset = drift_splitter(raw_data)\n",
    "    drift_report, _ = drift_detector(reference_dataset, comparison_dataset)\n",
    "    drift_alert(drift_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f67d2",
   "metadata": {},
   "source": [
    "### Configure the steps\n",
    "\n",
    "Now that we have mlflow enabled we need to choose what we want to log into mlflow. For now, we have chosen to use the [mlflow autolog](https://www.mlflow.org/docs/latest/tracking.html#scikit-learn) functionality to automatically log the model and training parameters within the training step.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> The @enable_mlflow decorator above the step is all we need to get started with mlflow. This decorator sets up an mlflow experiment and an mlflow backend for all runs within this pipeline. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a34a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from zenml.integrations.mlflow.mlflow_step_decorator import enable_mlflow\n",
    "import mlflow\n",
    "from zenml.client import Client\n",
    "from zenml.steps import step\n",
    "from zenml.steps.base_step_config import BaseStepConfig\n",
    "\n",
    "\n",
    "# This is how step configurations are defined\n",
    "class RandomForestTrainerConfig(BaseStepConfig):\n",
    "    \"\"\"Config class for the sklearn trainer\"\"\"\n",
    "    max_depth: int = 1000\n",
    "    target_col: str = 'FG3M'\n",
    "\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "@step(enable_cache=False, experiment_tracker=experiment_tracker.name)\n",
    "def random_forest_trainer(train_df_x: pd.DataFrame, train_df_y: pd.DataFrame,\n",
    "                          eval_df_x: pd.DataFrame, eval_df_y: pd.DataFrame,\n",
    "                          config: RandomForestTrainerConfig) -> RegressorMixin:\n",
    "\n",
    "    mlflow.sklearn.autolog()\n",
    "    clf = RandomForestRegressor(max_depth=config.max_depth)\n",
    "    clf.fit(train_df_x, np.squeeze(train_df_y.values.T))\n",
    "    eval_score = clf.score(eval_df_x, np.squeeze(eval_df_y.values.T))\n",
    "    print(f\"Eval score is: {eval_score}\")\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b461d2",
   "metadata": {},
   "source": [
    "Multiple of our steps have configurations that we want to set ahead of our pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zenml provides some standard steps for the evidently integration\n",
    "from zenml.integrations.evidently.steps import (\n",
    "    EvidentlyProfileConfig,\n",
    "    EvidentlyProfileStep,\n",
    ")\n",
    "from steps.splitter import SklearnSplitterConfig, TrainingSplitConfig\n",
    "\n",
    "# Here we simply choose how we will split our data\n",
    "train_data_split_config = SklearnSplitterConfig(\n",
    "    ratios={'train': 0.6, 'test': 0.2, 'validation': 0.2})\n",
    "\n",
    "# We have chosen to run the pipeline on a weekly schedule. As scuh we always want to look one week in the past \n",
    "#  and decide if the last week was anomalous​ in comparison to the last few years \n",
    "one_week_ago = (date.today() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "drift_data_split_config = TrainingSplitConfig(\n",
    "    new_data_split_date=one_week_ago,\n",
    "    start_reference_time_frame=CURRYS_THREE_POINTER,\n",
    "    end_reference_time_frame=one_week_ago,\n",
    "    columns=[\"FG3M\"])\n",
    "\n",
    "# Just like in the previous pipeline we choose the datadrift Profile\n",
    "evidently_profile_config = EvidentlyProfileConfig(\n",
    "    column_mapping=None,\n",
    "    profile_sections=[\"datadrift\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e2846",
   "metadata": {},
   "source": [
    "### Ready to run\n",
    "\n",
    "The pipeline is defined and our steps have been written. Let's instantiate our pipeline and start our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf796b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.analyzer import analyze_drift\n",
    "from steps.encoder import data_encoder\n",
    "from steps.evaluator import tester\n",
    "from steps.feature_engineer import feature_engineer\n",
    "from steps.importer import game_data_importer\n",
    "from steps.splitter import sklearn_splitter, SklearnSplitterConfig, reference_data_splitter, TrainingSplitConfig\n",
    "from steps.discord_bot import discord_alert\n",
    "\n",
    "\n",
    "# Initialize the pipeline\n",
    "train_pipeline = training_pipeline(\n",
    "    # Data Wrangling\n",
    "    importer=game_data_importer(),\n",
    "    feature_engineerer=feature_engineer(),\n",
    "    encoder=data_encoder(),\n",
    "    ml_splitter=sklearn_splitter(train_data_split_config),\n",
    "    \n",
    "    # Model training\n",
    "    trainer=random_forest_trainer(),\n",
    "    tester=tester(),\n",
    "    \n",
    "    # Drift detection\n",
    "    drift_splitter=reference_data_splitter(drift_data_split_config),\n",
    "    drift_detector=EvidentlyProfileStep(evidently_profile_config),\n",
    "    \n",
    "    # Alert Discord\n",
    "    drift_alert=discord_alert(),\n",
    ")\n",
    "\n",
    "train_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca539596",
   "metadata": {},
   "source": [
    "### Let's have a look at mlflow\n",
    "\n",
    "Training is done, let's have a look at our mlflow ui and see if our training including the model have made it in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e90efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "from zenml.integrations.mlflow.mlflow_environment import MLFLOW_ENVIRONMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b477d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will start a serving process for mlflow \n",
    "#  - if you want to continue in the notebook you need to manually\n",
    "#  interrupt the kernel \n",
    "from zenml.environment import Environment\n",
    "from zenml.integrations.mlflow.mlflow_environment import MLFLOW_ENVIRONMENT_NAME\n",
    "\n",
    "!mlflow ui --backend-store-uri {Environment()[MLFLOW_ENVIRONMENT_NAME].tracking_uri} --port 4998"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e752b",
   "metadata": {},
   "source": [
    "### Let's check out our Drift for this pipeline as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.visualizers import EvidentlyVisualizer\n",
    "from zenml.repository import Repository\n",
    "\n",
    "\n",
    "last_week = date.today() - timedelta(days=7)\n",
    "ONE_WEEK_AGO = last_week.strftime(\"%Y-%m-%d\")\n",
    "CURRY_FROM_DOWNTOWN = '2016-02-27'\n",
    "\n",
    "\n",
    "repo = Repository()\n",
    "p = repo.get_pipeline(pipeline_name='training_pipeline')\n",
    "last_run = p.runs[-1]\n",
    "drift_analysis_step = last_run.get_step(\n",
    "    name=\"drift_alert\"\n",
    ")\n",
    "print(f'Data drift detected: {drift_analysis_step.output.read()}')\n",
    "\n",
    "drift_detection_step = last_run.get_step(\n",
    "    name=\"drift_detector\"\n",
    ")\n",
    "evidently_outputs = drift_detection_step\n",
    "\n",
    "EvidentlyVisualizer().visualize(evidently_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b81607",
   "metadata": {},
   "source": [
    "## The ZenML stack\n",
    "\n",
    "The ZenML stack is a concept that describes the union of Metadata Store, Artifact Store and Orchestrator that will be used for all pipeline runs. When you get started with zenml you start off with a default local stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81cf8f",
   "metadata": {},
   "source": [
    "### The Local Stack\n",
    "\n",
    "You can imagine the local stack to look like this. Within the diagram we show how a generic pipeline interacts with the local stack.\n",
    "\n",
    "![LocalStack](_assets/localstack.png \"LocalStack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0475966",
   "metadata": {},
   "source": [
    "### The Kubeflow Pipeline stack\n",
    "\n",
    "Now we want to transition to a kubeflow stack that will look a little bit like this. Note that for kubeflow pipelines we also need a registry where the docker images for each step are registered. \n",
    "\n",
    "![KubeflowStack](_assets/localstack-with-kubeflow-orchestrator.png \"KubeflowStack\")\n",
    "\n",
    "But we have good news! You barely have to do anything to transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed715ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You register a container registry with zenml\n",
    "!zenml container-registry register local_registry  --flavor=default --uri=localhost:5000\n",
    "    \n",
    "# You register an orchestrator with zenml\n",
    "!zenml orchestrator register kubeflow_orchestrator  --flavor=kubeflow\n",
    "\n",
    "# Now it all is combined into the local_kubeflow_stack\n",
    "!zenml stack register local_kubeflow_stack \\\n",
    "    -a default \\\n",
    "    -o kubeflow_orchestrator \\\n",
    "    -c local_registry \\\n",
    "    -e local_mlflow_tracker \\\n",
    "    -dv local_evidently\n",
    "\n",
    "# And we activate the new stack, now all pipelines will be run within this stack\n",
    "!zenml stack set local_kubeflow_stack\n",
    "\n",
    "# Check it out, your new stack is registered\n",
    "!zenml stack list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793de856",
   "metadata": {},
   "source": [
    "### Starting up your new kubeflow pipelines stack\n",
    "\n",
    "All that is left to do is power up your stack. This is just one more line away. The stack up process might take some time for you. In the background k3d will be creating and starting up a cluster of docker containers to host kubeflow pipelines locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d2ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zenml stack up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989b88f",
   "metadata": {},
   "source": [
    "If you scroll down all the way on the previous output you should see a link to your running kubeflow pipelines UI. Most probably this will be at [http://localhost:8080/](http://localhost:8080/).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Currently running pipelines defined within a jupyter notebook cell is\n",
    "    not supported. To get around this you can run the train pipeline within this repo. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a2143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zenml stack set local_kubeflow_stack\n",
    "# Let's train within kubeflow pipelines - this will deploy the training pipeline on a Schedule\n",
    "!python run_pipeline.py train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf530aa6",
   "metadata": {},
   "source": [
    "# Chapter 3 - The Prediction Pipeline\n",
    "\n",
    "The Model is trained - time to get to the prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ee8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's return to our local stack first so we can continue within the jupyter notebook\n",
    "!zenml stack set local_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d6e37",
   "metadata": {},
   "source": [
    "This is the initial inference pipeline coupled with the training pipeline as described above.\n",
    "\n",
    "![Training And Inference Pipeline](_assets/Training%20and%20Inference%20Pipeline.png \"Planned Architecture Full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def inference_pipeline(\n",
    "        importer,           # Import the schedule for upcoming games\n",
    "        preprocessor,       # Preprocess data and use same encoder as the training data\n",
    "        extract_next_week,  # Extract only the next week of dat\n",
    "        model_picker,       # Pick the best model\n",
    "        predictor,          # Predict three pointers for home team\n",
    "        post_processor,     # Decode Encoded data and make human readable\n",
    "        prediction_poster   # Post Prediction to Discord\n",
    "):\n",
    "    \"\"\"Links all the steps together in a pipeline\"\"\"\n",
    "    season_schedule = importer()\n",
    "    processed_season_schedule, le_seasons = preprocessor(season_schedule)\n",
    "    upcoming_week = extract_next_week(processed_season_schedule)\n",
    "    model, run_id = model_picker()\n",
    "    predictions = predictor(model, upcoming_week, le_seasons)\n",
    "    readable_predictions = post_processor(predictions)\n",
    "    prediction_poster(readable_predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d33ef2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from steps.encoder import encode_columns_and_clean\n",
    "from steps.importer import import_season_schedule, SeasonScheduleConfig\n",
    "from steps.model_picker import model_picker\n",
    "from steps.predictor import predictor\n",
    "from steps.splitter import get_coming_week_data, TimeWindowConfig\n",
    "from steps.post_processor import data_post_processor\n",
    "from steps.discord_bot import discord_post_prediction\n",
    "\n",
    "# Initialize the pipeline\n",
    "inference_pipe = inference_pipeline(\n",
    "    importer=import_season_schedule(\n",
    "        SeasonScheduleConfig(current_season='2021-22')),\n",
    "    preprocessor=encode_columns_and_clean(),\n",
    "    extract_next_week=get_coming_week_data(TimeWindowConfig(time_window=7)),\n",
    "    model_picker=model_picker(),\n",
    "    predictor=predictor(),\n",
    "    post_processor=data_post_processor(),\n",
    "    prediction_poster=discord_post_prediction()\n",
    ")\n",
    "\n",
    "inference_pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b423e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at some of our predictions\n",
    "from zenml.repository import Repository\n",
    "\n",
    "r = Repository()\n",
    "df = r.get_pipeline(pipeline_name='inference_pipeline').runs[-1].steps[-2].output.read()\n",
    "df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0462ac95a48ad62a49dcdeb32c5f6c7ae6cba5ad1a7eea613ae2c2333897566f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
