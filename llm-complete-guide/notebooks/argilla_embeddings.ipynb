{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning custom embedding models on synthetic data\n",
    "\n",
    "Bootstrapping and maintaining production-ready RAG pipelines, requires optimising various components like the LLM, vector database, embeddings and rerankers. Within this notbeook we will showcase how you can optimize and maintain your embedding models through synthetic data and human feedback. Besides ZenML, we will do this by using two open source libraries: `argilla` and `distilabel`. Both of these libraries focus optimizing model outputs through improving data quality, however, each one of them take a diferent approach to tackle the same problem. `distilabel` provides a scalable and reliable approach to distilling knowledge from LLMs by generating synthetic data or providing AI feedback with LLMs as judges. `argilla` enables AI engineers and domain experts to collaborate on data projects by allowing them to organize and explore data through within an interactive and engagig UI. Both libraries can be used individually but they work better together.\n",
    "\n",
    "- ⚗️ distilabel is a framework for synthetic data and AI feedback - [docs](distilabel.argilla.io)\n",
    "- Argilla is a collaboration tool for AI engineers and domain experts to build high-quality datasets - [docs](docs.argilla.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset - vibe check\n",
    "\n",
    "Before starting any project, it is always important to look at your data. Our data is publicly [available on the Hugging Face Hub](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0) so we can have a quick look through their dataset viewer within an embedded iFrame. \n",
    "\n",
    "As we can see, our dataset contains a column called `page_content`, which was obtained from the ZenML docs.\n",
    "\n",
    "TODO: add context about `page_content`, chunking etc. Could we look into semantic chunking?\n",
    "\n",
    "<iframe src=\"https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0/embed/viewer\" frameborder=\"0\" width=\"100%\" height=\"560px\"></iframe>\n",
    "\n",
    "Alternatively, we can load the entire dataset to disk with `datasets.load_dataset`. There is only a single split (`train`), which we will provide as argument to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "repo_name = \"zenml/rag_qa_embedding_questions_0_60_0\"\n",
    "\n",
    "dataset = load_dataset(repo_name, split=\"train\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic query generation with `distilabel`\n",
    "\n",
    "The [`GenerateSentencePair`](https://distilabel.argilla.io/latest/components-gallery/tasks/generatesentencepair/) component from `distilabel` that can be used to generate training datasets for embeddings models. It is a pre-defined `Task` that given an `anchor` sentence generates a `positive` sentence related to the anchor. We will also generate unrelated `negative` sentences by passing `triplet=True` and we will also provide a `context` to guide the LLM towards more specific behavior. \n",
    "\n",
    "Additionally, we will use the [`OpenAILLM`](https://distilabel.argilla.io/latest/components-gallery/llms/openaillm/) with `gpt4o` and [`LoadDataFromHub`](https://distilabel.argilla.io/latest/components-gallery/steps/loaddatafromhub/) to load [our dataset](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0).\n",
    "\n",
    "In our case, we will use the `page_content` column from our dataset as `anchor` to generate `positive` and `negatives` sentences that function as training data for the embedding model.\n",
    "\n",
    "Now, let's capture this logic in a `distilabel` `Pipeline`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from distilabel.steps.tasks import GenerateSentencePair\n",
    "from distilabel.llms import OpenAILLM, OllamaLLM\n",
    "from distilabel.steps import LoadDataFromHub\n",
    "from distilabel.pipeline import Pipeline\n",
    "\n",
    "# TODO: I think we might optimize this a bit more.\n",
    "\n",
    "context = (\n",
    "\"\"\"\n",
    "The text is a chunk from technical documentation of ZenML.\n",
    "ZenML is an MLOps + LLMOps framework that makes your infrastructure and workflow metadata accessible to data science teams.\n",
    "Along with prose explanations, the text chunk may include code snippets and logs but these are identifiable from the surrounding backticks.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = OpenAILLM(model=\"gpt-4o\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "with Pipeline(name=\"generate_embedding_queries\") as pipeline:\n",
    "    load_dataset = LoadDataFromHub(\n",
    "        num_examples=15,  # uncomment this for demo purposes\n",
    "        output_mappings={\"page_content\": \"anchor\"},\n",
    "    )\n",
    "    generate_sentence_pair = GenerateSentencePair(\n",
    "        triplet=True,  # `False` to generate only positive\n",
    "        action=\"query\",\n",
    "        llm=llm,\n",
    "        input_batch_size=10,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "    load_dataset >> generate_sentence_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can execute this using `pipeline.run`. We will provide some `parameters` to specific components within our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_distiset = pipeline.run(  #\n",
    "    parameters={\n",
    "        load_dataset.name: {\n",
    "            \"repo_id\": \"zenml/rag_qa_embedding_questions_0_60_0\",\n",
    "            \"split\": \"train\",\n",
    "        },\n",
    "        generate_sentence_pair.name: {\n",
    "            \"llm\": {\n",
    "                \"generation_kwargs\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_new_tokens\": 512,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    use_cache=False, # comment out for demo\n",
    ")\n",
    "\n",
    "test_distiset = pipeline.run(  #\n",
    "    parameters={\n",
    "        load_dataset.name: {\n",
    "            \"repo_id\": \"zenml/rag_qa_embedding_questions_0_60_0\",\n",
    "            \"split\": \"test\",\n",
    "        },\n",
    "        generate_sentence_pair.name: {\n",
    "            \"llm\": {\n",
    "                \"generation_kwargs\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_new_tokens\": 512,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    use_cache=False, # comment out for demo\n",
    ")\n",
    "\n",
    "combined_distiset = DatasetDict({\n",
    "    \"train\": train_distiset[\"default\"][\"train\"],\n",
    "    \"test\": test_distiset[\"default\"][\"train\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vibe check our data again. If you are not happy with the results you can either tweak our `parameters` or optimize the `context` prompt which is passed to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vibe check our data again. If you are not happy with the results you can either tweak our `parameters` or optimize the `context` prompt which is passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "example = combined_distiset[\"train\"][9]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Push the distiset to the Hugging Face Hub\n",
    "\n",
    "Synthetic data generation can be expensive becuae of the reliance on LLMs, so first store our data on the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_distiset.push_to_hub(\n",
    "    repo_id=\"zenml/rag_qa_embedding_questions_0_60_0_distilabel\",\n",
    "    token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n",
    "    create_pr=True, # TODO: why?\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Review synthetic query generation with `argilla` \n",
    "\n",
    "Data is never as clean as it can be and this also holds for synthetically generated data, therefore, it is always good to spent some time and look at your data. We will used Argilla to do this. If you are not familiar with Argilla, we recommend taking a look at the [Argilla quickstar docs](https://docs.argilla.io/latest/getting_started/quickstart/). Alternatively, you can use your Hugging Face account to login to the [Argilla demo Space](https://argilla-argilla-template-space.hf.space).\n",
    "\n",
    "To start exploring data, we first need to define an `argilla.Dataset`. We will create a basic datset with some input `TextFields` for the `anchor` and output `TextQuestions` for the `positive` and `negative` pairs. Additionally, we will use the `parent_section` and `token_count` as `MetaDataProperty` and we will be adding some vectors to allow for semantic search. Finally, we will also compute current similarities for embeddings of `anchor-positive`, `positive-negative` and `anchor-negative` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611846370cd042148808cd29082246de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f69956fce94855af8a9493e901759e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/39.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513d55dab6644af5b89ceb9d5f7fb01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/35.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b400e2eab74a7c9df1319c02d7b100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80549fbccbb4e72a0236496369a579a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"zenml/rag_qa_embedding_questions_0_60_0_distilabel\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc19a82b63924e299c87ff188b663cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7435e9f3c4864983ade5fc2a65c0a7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf7e7c032764b828cfcdfd926bac3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2201cabc884878a7b6b623a613cff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f0101f029141d5b680a9f281201f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"  # Hugging Face model ID\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgillaError",
     "evalue": "Argilla SDK error: ArgillaError: Missing api_key. You must provide a valid API key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgillaError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margilla\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrg\u001b[39;00m\n\u001b[1;32m      3\u001b[0m settings \u001b[38;5;241m=\u001b[39m rg\u001b[38;5;241m.\u001b[39mSettings(\n\u001b[1;32m      4\u001b[0m     fields\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m----> 5\u001b[0m         \u001b[43mrg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextField\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manchor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[1;32m      7\u001b[0m     questions\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      8\u001b[0m         rg\u001b[38;5;241m.\u001b[39mTextQuestion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m         rg\u001b[38;5;241m.\u001b[39mTextQuestion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     ],\n\u001b[1;32m     11\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     12\u001b[0m         rg\u001b[38;5;241m.\u001b[39mTermsMetadataProperty(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_section\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     13\u001b[0m         rg\u001b[38;5;241m.\u001b[39mIntegerMetadataProperty(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     14\u001b[0m         rg\u001b[38;5;241m.\u001b[39mFloatMetadataProperty(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity-positive-negative\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m         rg\u001b[38;5;241m.\u001b[39mFloatMetadataProperty(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity-anchor-positive\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     16\u001b[0m         rg\u001b[38;5;241m.\u001b[39mFloatMetadataProperty(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity-anchor-negative\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     17\u001b[0m     ],\n\u001b[1;32m     18\u001b[0m     vectors\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     19\u001b[0m         rg\u001b[38;5;241m.\u001b[39mVectorField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manchor-vector\u001b[39m\u001b[38;5;124m\"\u001b[39m, dimensions\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_sentence_embedding_dimension())\n\u001b[1;32m     20\u001b[0m     ]\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m ds \u001b[38;5;241m=\u001b[39m rg\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m     23\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag_qa_embedding_questions_0_60_0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     settings\u001b[38;5;241m=\u001b[39msettings\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m ds\u001b[38;5;241m.\u001b[39mcreate()\n",
      "File \u001b[0;32m~/Documents/programming/argilla/collaborations/integrations/integration-zenml/.venv/lib/python3.9/site-packages/argilla/settings/_field.py:61\u001b[0m, in \u001b[0;36mTextField.__init__\u001b[0;34m(self, name, title, use_markdown, required, description, client)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     45\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     client: Optional[Argilla] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124;03m\"\"\"Text field for use in Argilla `Dataset` `Settings`\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m        name (str): The name of the field\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     client \u001b[38;5;241m=\u001b[39m client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mArgilla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(api\u001b[38;5;241m=\u001b[39mclient\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mfields, client\u001b[38;5;241m=\u001b[39mclient)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m FieldModel(\n\u001b[1;32m     65\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m     66\u001b[0m         title\u001b[38;5;241m=\u001b[39mtitle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m         settings\u001b[38;5;241m=\u001b[39mTextFieldSettings(use_markdown\u001b[38;5;241m=\u001b[39muse_markdown),\n\u001b[1;32m     70\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/programming/argilla/collaborations/integrations/integration-zenml/.venv/lib/python3.9/site-packages/argilla/client.py:100\u001b[0m, in \u001b[0;36mArgilla._get_default\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"Get the default instance of Argilla. If it doesn't exist, create a new one.\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_default_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_default_client \u001b[38;5;241m=\u001b[39m \u001b[43mArgilla\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_default_client\n",
      "File \u001b[0;32m~/Documents/programming/argilla/collaborations/integrations/integration-zenml/.venv/lib/python3.9/site-packages/argilla/client.py:62\u001b[0m, in \u001b[0;36mArgilla.__init__\u001b[0;34m(self, api_url, api_key, timeout, **http_client_args)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     57\u001b[0m     api_url: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m DEFAULT_HTTP_CONFIG\u001b[38;5;241m.\u001b[39mapi_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttp_client_args,\n\u001b[1;32m     61\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttp_client_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_default(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/programming/argilla/collaborations/integrations/integration-zenml/.venv/lib/python3.9/site-packages/argilla/_api/_client.py:114\u001b[0m, in \u001b[0;36mAPIClient.__init__\u001b[0;34m(self, api_url, api_key, timeout, **http_client_args)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArgillaError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing api_url. You must provide a valid API url\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArgillaError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing api_key. You must provide a valid API key.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url \u001b[38;5;241m=\u001b[39m api_url\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n",
      "\u001b[0;31mArgillaError\u001b[0m: Argilla SDK error: ArgillaError: Missing api_key. You must provide a valid API key."
     ]
    }
   ],
   "source": [
    "import argilla as rg\n",
    "\n",
    "client = rg.Argilla(api_url=\"https://davidberenstein1957-my-argilla.hf.space\", api_key=\"36aQq2_-ZqK6YTtzYKV1w_7lxkun0labHvs4xC7SgMPYU8IpSkY_1W2zJDWv7lSrMJBJLDBBey5XVYK08zfVZ26U0V-WaxHczmhp6IZGtoU\")\n",
    "settings = rg.Settings(\n",
    "    fields=[\n",
    "        rg.TextField(\"anchor\")\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.TextQuestion(\"positive\"),\n",
    "        rg.TextQuestion(\"negative\")\n",
    "    ],\n",
    "    metadata=[\n",
    "        rg.TermsMetadataProperty(\"parent_section\"),\n",
    "        rg.IntegerMetadataProperty(\"token_count\"),\n",
    "        rg.FloatMetadataProperty(\"similarity-positive-negative\"),\n",
    "        rg.FloatMetadataProperty(\"similarity-anchor-positive\"),\n",
    "        rg.FloatMetadataProperty(\"similarity-anchor-negative\"),\n",
    "    ],\n",
    "    vectors=[\n",
    "        rg.VectorField(\"anchor-vector\", dimensions=model.get_sentence_embedding_dimension())\n",
    "    ]\n",
    ")\n",
    "ds = rg.Dataset(\n",
    "    name=\"rag_qa_embedding_questions_0_60_0\",\n",
    "    settings=settings\n",
    ")\n",
    "ds.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will process the original Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b40b5ff5a5430da32ed19530e5ec4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def format_data(batch):\n",
    "    def get_embeddings(batch_column):\n",
    "        vectors = model.encode(batch_column)\n",
    "        return [vector.tolist() for vector in vectors]\n",
    "    batch[\"anchor-vector\"] = get_embeddings(batch[\"anchor\"])\n",
    "    batch[\"positive-vector\"] = get_embeddings(batch[\"positive\"])\n",
    "    batch[\"negative-vector\"] = get_embeddings(batch[\"negative\"])\n",
    "\n",
    "    def get_similarities(a, b):\n",
    "        similarities = []\n",
    "        \n",
    "        for pos_vec, neg_vec in zip(a, b):\n",
    "            similarity = cosine_similarity([pos_vec], [neg_vec])[0][0]\n",
    "            similarities.append(similarity)\n",
    "        return similarities\n",
    "    \n",
    "    batch[\"similarity-positive-negative\"] = get_similarities(batch[\"positive-vector\"], batch[\"negative-vector\"])\n",
    "    batch[\"similarity-anchor-positive\"] = get_similarities(batch[\"anchor-vector\"], batch[\"positive-vector\"])\n",
    "    batch[\"similarity-anchor-negative\"] = get_similarities(batch[\"anchor-vector\"], batch[\"negative-vector\"])\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(format_data, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will log the records to Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for idx, entry in enumerate(dataset):\n",
    "    records.append(\n",
    "        rg.Record(\n",
    "            id=idx,\n",
    "            fields={\"achor\": entry[\"anchor\"]},\n",
    "            suggestions=[\n",
    "                rg.Suggestion(\"positive\", value=entry[\"positive\"]),\n",
    "                rg.Suggestion(\"negative\", value=entry[\"negative\"]),\n",
    "            ],\n",
    "            metadata={\n",
    "                \"parent_section\": entry[\"parent_section\"],\n",
    "                \"token_count\": entry[\"token_count\"],\n",
    "                \"similarity-positive-negative\": entry[\"similarity-positive-negative\"],\n",
    "                \"similarity-anchor-positive\": entry[\"similarity-anchor-positive\"],\n",
    "                \"similarity-anchor-negative\": entry[\"similarity-anchor-negative\"]\n",
    "            },\n",
    "            vectors={\"question-vector\": entry[\"question-vector\"]}\n",
    "        )\n",
    "    )\n",
    "ds.records.log(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore the UI and filter out the bad apples. Tip, start filtering on high similarity of 'similarity-anchor-negative' or 'similarity-positive-negative' and low similarity of 'similarity-anchor-positive'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Review synthetic query generation with `argilla` \n",
    "\n",
    "Data is never as clean as it can be and this also holds for synthetically generated data, therefore, it is always good to spent some time and look at your data. We will used Argilla to do this. If you are not familiar with Argilla, we recommend taking a look at the [Argilla quickstart docs](https://docs.argilla.io/latest/getting_started/quickstart/). Alternatively, you can use your Hugging Face account to login to the [Argilla demo Space](https://argilla-argilla-template-space.hf.space).\n",
    "\n",
    "To start exploring data, we first need to define an `argilla.Dataset`. We will create a basic datset with some input `TextFields` for the `anchor` and output `TextQuestions` for the `positive` and `negative` pairs. Additionally, we will use the `parent_section` and `token_count` as `MetaDataProperty` and we will be adding some vectors to allow for semantic search. Finally, we will also compute current similarities for embeddings of `anchor-positive`, `positive-negative` and `anchor-negative` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"zenml/rag_qa_embedding_questions_0_60_0_distilabel\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"  # Hugging Face model ID\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = rg.Settings(\n",
    "    fields=[\n",
    "        rg.TextField(\"anchor\")\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.TextQuestion(\"positive\"),\n",
    "        rg.TextQuestion(\"negative\")\n",
    "    ],\n",
    "    metadata=[\n",
    "        rg.TermsMetadataProperty(\"parent_section\"),\n",
    "        rg.IntegerMetadataProperty(\"token_count\"),\n",
    "        rg.FloatMetadataProperty(\"similarity-positive-negative\"),\n",
    "        rg.FloatMetadataProperty(\"similarity-anchor-positive\"),\n",
    "        rg.FloatMetadataProperty(\"similarity-anchor-negative\"),\n",
    "    ],\n",
    "    vectors=[\n",
    "        rg.VectorField(\"anchor-vector\", dimensions=model.get_sentence_embedding_dimension())\n",
    "    ]\n",
    ")\n",
    "ds = rg.Dataset(\n",
    "    name=\"rag_qa_embedding_questions_0_60_0_distilabel\",\n",
    "    settings=settings\n",
    ")\n",
    "ds.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will process the original Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def format_data(batch):\n",
    "    def get_embeddings(batch_column):\n",
    "        vectors = model.encode(batch_column)\n",
    "        return [vector.tolist() for vector in vectors]\n",
    "    \n",
    "    batch[\"anchor-vector\"] = get_embeddings(batch[\"anchor\"])\n",
    "    batch[\"positive-vector\"] = get_embeddings(batch[\"positive\"])\n",
    "    batch[\"negative-vector\"] = get_embeddings(batch[\"negative\"])\n",
    "\n",
    "    def get_similarities(a, b):\n",
    "        similarities = []\n",
    "        \n",
    "        for pos_vec, neg_vec in zip(a, b):\n",
    "            similarity = cosine_similarity([pos_vec], [neg_vec])[0][0]\n",
    "            similarities.append(similarity)\n",
    "    \n",
    "    batch[\"similarity-positive-negative\"] = get_similarities(batch[\"positive-vector\"], batch[\"negative-vector\"])\n",
    "    batch[\"similarity-anchor-positive\"] = get_similarities(batch[\"anchor-vector\"], batch[\"positive-vector\"])\n",
    "    batch[\"similarity-anchor-negative\"] = get_similarities(batch[\"anchor-vector\"], batch[\"negative-vector\"])\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(format_data, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will log the records to Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for idx, entry in enumerate(dataset):\n",
    "    records.append(\n",
    "        rg.Record(\n",
    "            id=idx,\n",
    "            fields={\"anchor\": entry[\"anchor\"]},\n",
    "            suggestions=[\n",
    "                rg.Suggestion(\"positive\", value=entry[\"positive\"]),\n",
    "                rg.Suggestion(\"negative\", value=entry[\"negative\"]),\n",
    "            ],\n",
    "            metadata={\n",
    "                \"parent_section\": entry[\"parent_section\"],\n",
    "                \"token_count\": entry[\"token_count\"],\n",
    "                \"similarity-positive-negative\": entry[\"similarity-positive-negative\"],\n",
    "                \"similarity-anchor-positive\": entry[\"similarity-anchor-positive\"],\n",
    "                \"similarity-anchor-negative\": entry[\"similarity-anchor-negative\"]\n",
    "            },\n",
    "            vectors={\"question-vector\": entry[\"question-vector\"]}\n",
    "        )\n",
    "    )\n",
    "ds.records.log(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore the UI and filter out the bad apples. Tip, start filtering on high similarity of 'similarity-anchor-negative' or 'similarity-positive-negative' and low similarity of 'similarity-anchor-positive'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the embedding dataset\n",
    "\n",
    "Follows [Phil Schmid's tutorial](https://www.philschmid.de/fine-tune-embedding-model-for-rag#5-evaluate-fine-tuned-model-against-baseline) fairly heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"zenml/rag_qa_embedding_questions_0_60_0\", split=\"train\")\n",
    "\n",
    "# Add an id column to the dataset\n",
    "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "\n",
    "# split dataset into a 10% test set\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset_path = \"../data/train_dataset.json\"\n",
    "test_dataset_path = \"../data/test_dataset.json\"\n",
    "\n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(train_dataset_path, orient=\"records\")\n",
    "dataset[\"test\"].to_json(test_dataset_path, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create baseline + evaluate pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator,\n",
    "    SequentialEvaluator,\n",
    ")\n",
    "from sentence_transformers.util import cos_sim\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"  # Hugging Face model ID\n",
    "matryoshka_dimensions = [384, 256, 128, 64]  # Important: large to small\n",
    "\n",
    "# Load a model\n",
    "model = SentenceTransformer(\n",
    "    model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# load test dataset\n",
    "test_dataset = load_dataset(\"json\", data_files=test_dataset_path, split=\"train\")\n",
    "train_dataset = load_dataset(\"json\", data_files=train_dataset_path, split=\"train\")\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "\n",
    "# Convert the datasets to dictionaries\n",
    "corpus = dict(\n",
    "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
    ")  # Our corpus (cid => document)\n",
    "queries = dict(\n",
    "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
    ")  # Our queries (qid => question)\n",
    "\n",
    "# Create a mapping of relevant document (1 in our case) for each query\n",
    "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "for q_id in queries:\n",
    "    relevant_docs[q_id] = [q_id]\n",
    "\n",
    "\n",
    "matryoshka_evaluators = []\n",
    "# Iterate over the different dimensions\n",
    "for dim in matryoshka_dimensions:\n",
    "    ir_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=queries,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs,\n",
    "        name=f\"dim_{dim}\",\n",
    "        truncate_dim=dim,  # Truncate the embeddings to a certain dimension\n",
    "        score_functions={\"cosine\": cos_sim},\n",
    "    )\n",
    "    matryoshka_evaluators.append(ir_evaluator)\n",
    "\n",
    "# Create a sequential evaluator\n",
    "evaluator = SequentialEvaluator(matryoshka_evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = evaluator(model)\n",
    "\n",
    "# # COMMENT IN for full results\n",
    "# print(results)\n",
    "\n",
    "# Print the main score\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    print\n",
    "    print(f\"{key}: {results[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerModelCardData, SentenceTransformer\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# load model with SDPA for using Flash Attention 2\n",
    "model = SentenceTransformer(\n",
    "    model_id,\n",
    "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"BGE base Financial Matryoshka\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "matryoshka_dimensions = [384, 256, 128, 64]  # Important: large to small\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "# load train dataset again\n",
    "train_dataset = load_dataset(\"json\", data_files=train_dataset_path, split=\"train\")\n",
    "\n",
    "# define training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"bge-base-financial-matryoshka\",  # output directory and hugging face model ID\n",
    "    num_train_epochs=4,  # number of epochs\n",
    "    per_device_train_batch_size=32,  # train batch size\n",
    "    gradient_accumulation_steps=16,  # for a global batch size of 512\n",
    "    per_device_eval_batch_size=16,  # evaluation batch size\n",
    "    warmup_ratio=0.1,  # warmup ratio\n",
    "    learning_rate=2e-5,  # learning rate, 2e-5 is a good value\n",
    "    lr_scheduler_type=\"cosine\",  # use constant learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",  # use fused adamw optimizer\n",
    "    tf32=True,  # use tf32 precision\n",
    "    bf16=True,  # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    eval_strategy=\"epoch\",  # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # save after each epoch\n",
    "    logging_steps=10,  # log every 10 steps\n",
    "    save_total_limit=3,  # save only the last 3 models\n",
    "    load_best_model_at_end=True,  # load the best model when training ends\n",
    "    metric_for_best_model=\"eval_dim_128_cosine_ndcg@10\",  # Optimizing for the best ndcg@10 score for the 128 dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,  # training arguments\n",
    "    train_dataset=train_dataset.select_columns(\n",
    "        [\"positive\", \"anchor\"]\n",
    "    ),  # training dataset\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save the best model\n",
    "trainer.save_model()\n",
    "\n",
    "# push model to hub\n",
    "# trainer.model.push_to_hub(\"bge-base-financial-matryoshka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate fine-tuned model against baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "fine_tuned_model = SentenceTransformer(\n",
    "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "# Evaluate the model\n",
    "results = evaluator(fine_tuned_model)\n",
    "\n",
    "# # COMMENT IN for full results\n",
    "print(results)\n",
    "\n",
    "# Print the main score\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    print(f\"{key}: {results[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
