{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning custom embedding models on synthetic data\n",
    "\n",
    "Bootstrapping and maintaining production-ready RAG pipelines, requires optimising various components like the LLM, vector database, embeddings and rerankers. Within this notbeook we will showcase how you can optimize and maintain your embedding models through synthetic data and human feedback. Besides ZenML, we will do this by using two open source libraries: `argilla` and `distilabel`. Both of these libraries focus optimizing model outputs through improving data quality, however, each one of them take a diferent approach to tackle the same problem. `distilabel` provides a scalable and reliable approach to distilling knowledge from LLMs by generating synthetic data or providing AI feedback with LLMs as judges. `argilla` enables AI engineers and domain experts to collaborate on data projects by allowing them to organize and explore data through within an interactive and engagig UI. Both libraries can be used individually but they work better together.\n",
    "\n",
    "- ⚗️ distilabel is a framework for synthetic data and AI feedback - [docs](distilabel.argilla.io)\n",
    "- Argilla is a collaboration tool for AI engineers and domain experts to build high-quality datasets - [docs](docs.argilla.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset - vibe check\n",
    "\n",
    "Before starting any project, it is always important to look at your data. Our data is publicly [available on the Hugging Face Hub](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0) so we can have a quick look through [their dataset viewer within an embedded iFrame](https://huggingface.co/docs/hub/datasets-viewer-embed). \n",
    "\n",
    "As we can see, our dataset contains a column called `page_content`, which was obtained from the ZenML docs.\n",
    "\n",
    "TODO: add context about `page_content`, chunking etc. Could we look into semantic chunking?\n",
    "\n",
    "<iframe src=\"https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0/embed/viewer\" frameborder=\"0\" width=\"100%\" height=\"560px\"></iframe>\n",
    "\n",
    "Alternatively, we can load the entire dataset to disk with `datasets.load_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "repo_name = \"zenml/rag_qa_embedding_questions_0_60_0\"\n",
    "\n",
    "dataset = load_dataset(repo_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic query generation with `distilabel`\n",
    "\n",
    "The [`GenerateSentencePair`](https://distilabel.argilla.io/latest/components-gallery/tasks/generatesentencepair/) component from `distilabel` that can be used to generate training datasets for embeddings models. It is a pre-defined `Task` that given an `anchor` sentence generates a `positive` sentence related to the anchor. We will also generate unrelated `negative` sentences by passing `triplet=True` and we will also provide a `context` to guide the LLM towards more specific behavior. \n",
    "\n",
    "Additionally, we will use the [`OpenAILLM`](https://distilabel.argilla.io/latest/components-gallery/llms/openaillm/) with `gpt4o` and [`LoadDataFromHub`](https://distilabel.argilla.io/latest/components-gallery/steps/loaddatafromhub/) to load [our dataset](https://huggingface.co/datasets/zenml/rag_qa_embedding_questions_0_60_0).\n",
    "\n",
    "In our case, we will use the `page_content` column from our dataset as `anchor` to generate `positive` and `negatives` sentences that function as training data for the embedding model.\n",
    "\n",
    "Now, let's capture this logic in a `distilabel` `Pipeline`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from distilabel.steps.tasks import GenerateSentencePair\n",
    "from distilabel.llms import OpenAILLM, OllamaLLM\n",
    "from distilabel.steps import LoadDataFromHub\n",
    "from distilabel.pipeline import Pipeline\n",
    "\n",
    "# TODO: I think we might optimize this a bit more.\n",
    "\n",
    "context = (\n",
    "\"\"\"\n",
    "The text is a chunk from technical documentation of ZenML.\n",
    "ZenML is an MLOps + LLMOps framework that makes your infrastructure and workflow metadata accessible to data science teams.\n",
    "Along with prose explanations, the text chunk may include code snippets and logs but these are identifiable from the surrounding backticks.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "llm = OpenAILLM(model=\"gpt-4o\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "with Pipeline(name=\"generate_embedding_queries\") as pipeline:\n",
    "    load_dataset = LoadDataFromHub(\n",
    "        num_examples=15,  # uncomment this for demo purposes\n",
    "        output_mappings={\"page_content\": \"anchor\"},\n",
    "    )\n",
    "    generate_sentence_pair = GenerateSentencePair(\n",
    "        triplet=True,  # `False` to generate only positive\n",
    "        action=\"query\",\n",
    "        llm=llm,\n",
    "        input_batch_size=10,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "    load_dataset >> generate_sentence_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can execute this using `pipeline.run`. We will provide some `parameters` to specific components within our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train_distiset = pipeline.run(  #\n",
    "    parameters={\n",
    "        load_dataset.name: {\n",
    "            \"repo_id\": \"zenml/rag_qa_embedding_questions_0_60_0\",\n",
    "            \"split\": \"train\",\n",
    "        },\n",
    "        generate_sentence_pair.name: {\n",
    "            \"llm\": {\n",
    "                \"generation_kwargs\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_new_tokens\": 512,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    use_cache=False, # comment out for demo\n",
    ")\n",
    "\n",
    "test_distiset = pipeline.run(  #\n",
    "    parameters={\n",
    "        load_dataset.name: {\n",
    "            \"repo_id\": \"zenml/rag_qa_embedding_questions_0_60_0\",\n",
    "            \"split\": \"test\",\n",
    "        },\n",
    "        generate_sentence_pair.name: {\n",
    "            \"llm\": {\n",
    "                \"generation_kwargs\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"max_new_tokens\": 512,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    use_cache=False, # comment out for demo\n",
    ")\n",
    "\n",
    "combined_distiset = DatasetDict({\n",
    "    \"train\": train_distiset[\"default\"][\"train\"],\n",
    "    \"test\": test_distiset[\"default\"][\"train\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vibe check our data again. If you are not happy with the results you can either tweak our `parameters` or optimize the `context` prompt which is passed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "\n",
    "example = combined_distiset[\"train\"][9]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Push the distiset to the Hugging Face Hub\n",
    "\n",
    "Synthetic data generation can be expensive becuae of the reliance on LLMs, so first store our data on the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_distiset.push_to_hub(\n",
    "    repo_id=\"zenml/rag_qa_embedding_questions_0_60_0_distilabel\",\n",
    "    token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Review synthetic query generation with `argilla` \n",
    "\n",
    "Data is never as clean as it can be and this also holds for synthetically generated data, therefore, it is always good to spent some time and look at your data. We will used Argilla to do this. If you are not familiar with Argilla, we recommend taking a look at the [Argilla quickstart docs](https://docs.argilla.io/latest/getting_started/quickstart/). Alternatively, you can use your Hugging Face account to login to the [Argilla demo Space](https://argilla-argilla-template-space.hf.space).\n",
    "\n",
    "To start exploring data, we first need to define an `argilla.Dataset`. We will create a basic datset with some input `TextFields` for the `anchor` and output `TextQuestions` for the `positive` and `negative` pairs. Additionally, we will use the `parent_section` and `token_count` as `MetaDataProperty` and we will be adding some vectors to allow for semantic search. Finally, we will also compute current similarities for embeddings of `anchor-positive`, `positive-negative` and `anchor-negative` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"zenml/rag_qa_embedding_questions_0_60_0_distilabel\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"  # Hugging Face model ID\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import argilla as rg\n",
    "\n",
    "client = rg.Argilla(api_url=os.getenv(\"ARGILLA_API_URL\"), api_key=os.getenv(\"ARGILLA_API_KEY\"))\n",
    "\n",
    "settings = rg.Settings(\n",
    "    fields=[\n",
    "        rg.TextField(\"anchor\")\n",
    "    ],\n",
    "    questions=[\n",
    "        rg.TextQuestion(\"positive\"),\n",
    "        rg.TextQuestion(\"negative\")\n",
    "    ],\n",
    "    metadata=[\n",
    "        rg.TermsMetadataProperty(\"parent_section\"),\n",
    "        rg.IntegerMetadataProperty(\"token_count\"),\n",
    "        rg.FloatMetadataProperty(\"similarity-positive-negative\"),\n",
    "        rg.FloatMetadataProperty(\"similarity-anchor-positive\"),\n",
    "        rg.FloatMetadataProperty(\"similarity-anchor-negative\"),\n",
    "    ],\n",
    "    vectors=[\n",
    "        rg.VectorField(\"anchor-vector\", dimensions=model.get_sentence_embedding_dimension())\n",
    "    ]\n",
    ")\n",
    "ds = rg.Dataset(\n",
    "    name=\"rag_qa_embedding_questions_0_60_0_distilabel\",\n",
    "    settings=settings\n",
    ")\n",
    "ds.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will process the original Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def format_data(batch):\n",
    "    def get_embeddings(batch_column):\n",
    "        vectors = model.encode(batch_column)\n",
    "        return [vector.tolist() for vector in vectors]\n",
    "    batch[\"anchor-vector\"] = get_embeddings(batch[\"anchor\"])\n",
    "    batch[\"positive-vector\"] = get_embeddings(batch[\"positive\"])\n",
    "    batch[\"negative-vector\"] = get_embeddings(batch[\"negative\"])\n",
    "\n",
    "    def get_similarities(a, b):\n",
    "        similarities = []\n",
    "        \n",
    "        for pos_vec, neg_vec in zip(a, b):\n",
    "            similarity = cosine_similarity([pos_vec], [neg_vec])[0][0]\n",
    "            similarities.append(similarity)\n",
    "        return similarities\n",
    "    \n",
    "    batch[\"similarity-positive-negative\"] = get_similarities(batch[\"positive-vector\"], batch[\"negative-vector\"])\n",
    "    batch[\"similarity-anchor-positive\"] = get_similarities(batch[\"anchor-vector\"], batch[\"positive-vector\"])\n",
    "    batch[\"similarity-anchor-negative\"] = get_similarities(batch[\"anchor-vector\"], batch[\"negative-vector\"])\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(format_data, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will log the records to Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for idx, entry in enumerate(dataset):\n",
    "    records.append(\n",
    "        rg.Record(\n",
    "            id=idx,\n",
    "            fields={\"anchor\": entry[\"anchor\"]},\n",
    "            suggestions=[\n",
    "                rg.Suggestion(\"positive\", value=entry[\"positive\"]),\n",
    "                rg.Suggestion(\"negative\", value=entry[\"negative\"]),\n",
    "            ],\n",
    "            metadata={\n",
    "                \"parent_section\": entry[\"parent_section\"],\n",
    "                \"token_count\": entry[\"token_count\"],\n",
    "                \"similarity-positive-negative\": entry[\"similarity-positive-negative\"],\n",
    "                \"similarity-anchor-positive\": entry[\"similarity-anchor-positive\"],\n",
    "                \"similarity-anchor-negative\": entry[\"similarity-anchor-negative\"]\n",
    "            },\n",
    "            vectors={\"anchor-vector\": entry[\"anchor-vector\"]}\n",
    "        )\n",
    "    )\n",
    "ds.records.log(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore the UI and filter out the bad apples. Tip, start filtering on high similarity of 'similarity-anchor-negative' or 'similarity-positive-negative' and low similarity of 'similarity-anchor-positive'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the embedding dataset\n",
    "\n",
    "Follows [Phil Schmid's tutorial](https://www.philschmid.de/fine-tune-embedding-model-for-rag#5-evaluate-fine-tuned-model-against-baseline) fairly heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"zenml/rag_qa_embedding_questions_0_60_0\", split=\"train\")\n",
    "\n",
    "# Add an id column to the dataset\n",
    "dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "\n",
    "# split dataset into a 10% test set\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset_path = \"../data/train_dataset.json\"\n",
    "test_dataset_path = \"../data/test_dataset.json\"\n",
    "\n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(train_dataset_path, orient=\"records\")\n",
    "dataset[\"test\"].to_json(test_dataset_path, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create baseline + evaluate pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator,\n",
    "    SequentialEvaluator,\n",
    ")\n",
    "from sentence_transformers.util import cos_sim\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"  # Hugging Face model ID\n",
    "matryoshka_dimensions = [384, 256, 128, 64]  # Important: large to small\n",
    "\n",
    "# Load a model\n",
    "model = SentenceTransformer(\n",
    "    model_id, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# load test dataset\n",
    "test_dataset = load_dataset(\"json\", data_files=test_dataset_path, split=\"train\")\n",
    "train_dataset = load_dataset(\"json\", data_files=train_dataset_path, split=\"train\")\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "\n",
    "# Convert the datasets to dictionaries\n",
    "corpus = dict(\n",
    "    zip(corpus_dataset[\"id\"], corpus_dataset[\"positive\"])\n",
    ")  # Our corpus (cid => document)\n",
    "queries = dict(\n",
    "    zip(test_dataset[\"id\"], test_dataset[\"anchor\"])\n",
    ")  # Our queries (qid => question)\n",
    "\n",
    "# Create a mapping of relevant document (1 in our case) for each query\n",
    "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "for q_id in queries:\n",
    "    relevant_docs[q_id] = [q_id]\n",
    "\n",
    "\n",
    "matryoshka_evaluators = []\n",
    "# Iterate over the different dimensions\n",
    "for dim in matryoshka_dimensions:\n",
    "    ir_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=queries,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs,\n",
    "        name=f\"dim_{dim}\",\n",
    "        truncate_dim=dim,  # Truncate the embeddings to a certain dimension\n",
    "        score_functions={\"cosine\": cos_sim},\n",
    "    )\n",
    "    matryoshka_evaluators.append(ir_evaluator)\n",
    "\n",
    "# Create a sequential evaluator\n",
    "evaluator = SequentialEvaluator(matryoshka_evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = evaluator(model)\n",
    "\n",
    "# # COMMENT IN for full results\n",
    "# print(results)\n",
    "\n",
    "# Print the main score\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    print\n",
    "    print(f\"{key}: {results[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerModelCardData, SentenceTransformer\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# load model with SDPA for using Flash Attention 2\n",
    "model = SentenceTransformer(\n",
    "    model_id,\n",
    "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
    "    model_card_data=SentenceTransformerModelCardData(\n",
    "        language=\"en\",\n",
    "        license=\"apache-2.0\",\n",
    "        model_name=\"BGE base Financial Matryoshka\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "matryoshka_dimensions = [384, 256, 128, 64]  # Important: large to small\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "# load train dataset again\n",
    "train_dataset = load_dataset(\"json\", data_files=train_dataset_path, split=\"train\")\n",
    "\n",
    "# define training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"bge-base-financial-matryoshka\",  # output directory and hugging face model ID\n",
    "    num_train_epochs=4,  # number of epochs\n",
    "    per_device_train_batch_size=32,  # train batch size\n",
    "    gradient_accumulation_steps=16,  # for a global batch size of 512\n",
    "    per_device_eval_batch_size=16,  # evaluation batch size\n",
    "    warmup_ratio=0.1,  # warmup ratio\n",
    "    learning_rate=2e-5,  # learning rate, 2e-5 is a good value\n",
    "    lr_scheduler_type=\"cosine\",  # use constant learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",  # use fused adamw optimizer\n",
    "    tf32=True,  # use tf32 precision\n",
    "    bf16=True,  # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    eval_strategy=\"epoch\",  # evaluate after each epoch\n",
    "    save_strategy=\"epoch\",  # save after each epoch\n",
    "    logging_steps=10,  # log every 10 steps\n",
    "    save_total_limit=3,  # save only the last 3 models\n",
    "    load_best_model_at_end=True,  # load the best model when training ends\n",
    "    metric_for_best_model=\"eval_dim_128_cosine_ndcg@10\",  # Optimizing for the best ndcg@10 score for the 128 dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,  # training arguments\n",
    "    train_dataset=train_dataset.select_columns(\n",
    "        [\"positive\", \"anchor\"]\n",
    "    ),  # training dataset\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save the best model\n",
    "trainer.save_model()\n",
    "\n",
    "# push model to hub\n",
    "# trainer.model.push_to_hub(\"bge-base-financial-matryoshka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate fine-tuned model against baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "fine_tuned_model = SentenceTransformer(\n",
    "    args.output_dir, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "# Evaluate the model\n",
    "results = evaluator(fine_tuned_model)\n",
    "\n",
    "# # COMMENT IN for full results\n",
    "print(results)\n",
    "\n",
    "# Print the main score\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    print(f\"{key}: {results[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
