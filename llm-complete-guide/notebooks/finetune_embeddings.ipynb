{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %uv pip install sentence-transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "## Step 1: use an existing language model\n",
    "word_embedding_model = models.Transformer(\"distilroberta-base\")\n",
    "\n",
    "## Step 2: use a pool function over the token embeddings\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension()\n",
    ")\n",
    "\n",
    "## Join steps 1 and 2 using the modules argument\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push a train-test split for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"zenml/rag_qa_embedding_questions\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from zenml.client import Client\n",
    "\n",
    "# Assuming you have a Dataset object named 'dataset'\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Create a DatasetDict with train and test splits\n",
    "dataset_dict = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_test_split[\"train\"],\n",
    "        \"test\": train_test_split[\"test\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Upload the dataset to the same repository on the Hugging Face Hub as a new branch\n",
    "client = Client()\n",
    "hf_token = client.get_secret(\"huggingface_datasets\").secret_values[\"token\"]\n",
    "\n",
    "branch_name = \"train_test_split\"\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    repo_id=dataset_name,\n",
    "    private=True,\n",
    "    token=hf_token,\n",
    "    branch=branch_name,\n",
    "    create_pr=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "modelB = SentenceTransformer(\n",
    "    \"embedding-data/distilroberta-base-sentence-transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"zenml/rag_qa_embedding_questions\"\n",
    "datasetB = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import inspect\n",
    "\n",
    "inspect(datasetB[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetB.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "train_examplesB = []\n",
    "train_dataB = datasetB\n",
    "n_examples = datasetB.num_rows\n",
    "\n",
    "for i in range(n_examples):\n",
    "    example = train_dataB[i]\n",
    "    train_examplesB.append(\n",
    "        InputExample(\n",
    "            texts=[example[\"generated_questions\"][0], example[\"page_content\"]]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloaderB = DataLoader(train_examplesB, shuffle=True, batch_size=64)\n",
    "train_lossB = losses.MultipleNegativesRankingLoss(model=modelB)\n",
    "num_epochsB = 10\n",
    "warmup_stepsB = int(\n",
    "    len(train_dataloaderB) * num_epochsB * 0.1\n",
    ")  # 10% of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB.fit(\n",
    "    train_objectives=[(train_dataloaderB, train_lossB)],\n",
    "    epochs=num_epochsB,\n",
    "    warmup_steps=warmup_stepsB,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question texts: ('Question 1', 'Question 2')\n",
      "Context texts: ('Context 1', 'Context 2')\n",
      "---\n",
      "Question texts: ('Question 3', 'Question 4')\n",
      "Context texts: ('Context 3', 'Context 4')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class InputExampleDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            return [self.__getitem__(i) for i in idx]\n",
    "        example = self.examples[idx]\n",
    "        return example.texts[0], example.texts[1]\n",
    "\n",
    "\n",
    "# Create some sample InputExamples\n",
    "examples = [\n",
    "    InputExample(texts=[\"Question 1\", \"Context 1\"]),\n",
    "    InputExample(texts=[\"Question 2\", \"Context 2\"]),\n",
    "    InputExample(texts=[\"Question 3\", \"Context 3\"]),\n",
    "    InputExample(texts=[\"Question 4\", \"Context 4\"]),\n",
    "]\n",
    "\n",
    "# Create an instance of InputExampleDataset\n",
    "dataset = InputExampleDataset(examples)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Iterate over the batches\n",
    "for batch in dataloader:\n",
    "    question_texts, context_texts = batch\n",
    "    print(\"Question texts:\", question_texts)\n",
    "    print(\"Context texts:\", context_texts)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft-embed-hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
