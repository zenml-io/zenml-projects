{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Classification Pipeline with ZenML\n",
    "\n",
    "This notebook demonstrates a ZenML pipeline for iris classification, including data loading, model training, evaluation, explainability, and data drift detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml connect --url=https://d13d987c-zenml.cloudinfra.zenml.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Client().activate_stack(\"default_with_s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[32m⠋\u001b[0m Describing the stack...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[3m              Stack Configuration               \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mCOMPONENT_TYPE    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mCOMPONENT_NAME         \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┠────────────────────┼─────────────────────────┨\n",
      "┃ ORCHESTRATOR       │ default                 ┃\n",
      "┠────────────────────┼─────────────────────────┨\n",
      "┃ STEP_OPERATOR      │ aws-sagemaker-pipelines ┃\n",
      "┠────────────────────┼─────────────────────────┨\n",
      "┃ ARTIFACT_STORE     │ aws-sagemaker-pipelines ┃\n",
      "┠────────────────────┼─────────────────────────┨\n",
      "┃ CONTAINER_REGISTRY │ aws-sagemaker-pipelines ┃\n",
      "┠────────────────────┼─────────────────────────┨\n",
      "┃ IMAGE_BUILDER      │ aws-sagemaker-pipelines ┃\n",
      "┗━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
      "\u001b[2;3m        'local-aws-step-operator' stack         \u001b[0m\n",
      "\u001b[32m⠙\u001b[0m Describing the stack...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[3m           Labels           \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━┯━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLABEL           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mVALUE\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┠──────────────────┼───────┨\n",
      "┃ zenml:full_stack │ True  ┃\n",
      "┗━━━━━━━━━━━━━━━━━━┷━━━━━━━┛\n",
      "\u001b[32m⠙\u001b[0m Describing the stack...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mStack \u001b[0m\u001b[2;32m'local-aws-step-operator'\u001b[0m\u001b[2;36m with id \u001b[0m\u001b[2;32m'1a600008-41e2-448e-b71f-5f525b564730'\u001b[0m\u001b[2;36m \u001b[0m\n",
      "\u001b[2;36mis owned by user stefan@zenml.io.\u001b[0m\n",
      "\u001b[2;32m⠙\u001b[0m\u001b[2;36m Describing the stack...\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Describing the stack...\n",
      "\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mDashboard URL: \u001b[0m\n",
      "\u001b[2;4;94mhttps://d13d987c-zenml.cloudinfra.zenml.io/workspaces/default/stacks/1a600008-41\u001b[0m\n",
      "\u001b[2;4;94me2-448e-b71f-5f525b564730/configuration\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!zenml stack describe 'local-aws-step-operator'\n",
    "Client().activate_stack(\"local-aws-step-operator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zenml stack describe 'aws-sagemaker-pipelines'\n",
    "# Client().activate_stack('aws-sagemaker-pipelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing_extensions import Annotated\n",
    "from zenml import log_artifact_metadata, step\n",
    "\n",
    "\n",
    "def safe_metadata(data: Any) -> Dict[str, Any]:\n",
    "    \"\"\"Create metadata dict with only supported types.\"\"\"\n",
    "    metadata = {\"shape\": data.shape}\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        metadata[\"columns\"] = list(data.columns)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "@step\n",
    "def load_data() -> (\n",
    "    Tuple[\n",
    "        Annotated[pd.DataFrame, \"X_train\"],\n",
    "        Annotated[pd.DataFrame, \"X_test\"],\n",
    "        Annotated[pd.Series, \"y_train\"],\n",
    "        Annotated[pd.Series, \"y_test\"],\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Load the iris dataset and split into train and test sets.\"\"\"\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    for name, data in [\n",
    "        (\"X_train\", X_train),\n",
    "        (\"X_test\", X_test),\n",
    "        (\"y_train\", y_train),\n",
    "        (\"y_test\", y_test),\n",
    "    ]:\n",
    "        log_artifact_metadata(\n",
    "            artifact_name=name, metadata={\"dataset_info\": safe_metadata(data)}\n",
    "        )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from typing_extensions import Annotated\n",
    "from zenml import (\n",
    "    ArtifactConfig,\n",
    "    log_artifact_metadata,\n",
    "    log_model_metadata,\n",
    "    step,\n",
    ")\n",
    "from zenml.integrations.aws.flavors.sagemaker_step_operator_flavor import (\n",
    "    SagemakerStepOperatorSettings,\n",
    ")\n",
    "\n",
    "\n",
    "@step(\n",
    "    enable_cache=False,\n",
    "    step_operator=\"aws-sagemaker-pipelines\",\n",
    "    settings={\n",
    "        \"step_operator.sagemaker\": SagemakerStepOperatorSettings(\n",
    "            estimator_args={\"instance_type\": \"ml.p3.2xlarge\"}\n",
    "        )\n",
    "    },\n",
    ")\n",
    "def train_model(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> Annotated[SVC, ArtifactConfig(name=\"model\", is_model_artifact=True)]:\n",
    "    \"\"\"Train an SVM classifier.\"\"\"\n",
    "    model = SVC(kernel=\"rbf\", probability=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_accuracy = model.score(X_train, y_train)\n",
    "\n",
    "    log_model_metadata(\n",
    "        metadata={\n",
    "            \"training_metrics\": {\n",
    "                \"train_accuracy\": float(train_accuracy),\n",
    "            },\n",
    "            \"model_info\": {\n",
    "                \"model_type\": type(model).__name__,\n",
    "                \"kernel\": model.kernel,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    log_artifact_metadata(\n",
    "        artifact_name=\"model\",\n",
    "        metadata={\n",
    "            \"model_details\": {\n",
    "                \"type\": type(model).__name__,\n",
    "                \"kernel\": model.kernel,\n",
    "                \"n_support\": model.n_support_.tolist(),\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from typing_extensions import Annotated\n",
    "from zenml import log_artifact_metadata, log_model_metadata, step\n",
    "\n",
    "\n",
    "@step\n",
    "def evaluate_model(\n",
    "    model: SVC,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    ") -> Tuple[\n",
    "    Annotated[np.ndarray, \"predictions\"],\n",
    "    Annotated[np.ndarray, \"probabilities\"],\n",
    "]:\n",
    "    \"\"\"Evaluate the model and make predictions.\"\"\"\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    predictions = model.predict(X_test)\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "\n",
    "    log_model_metadata(\n",
    "        metadata={\n",
    "            \"evaluation_metrics\": {\n",
    "                \"test_accuracy\": float(test_accuracy),\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    log_artifact_metadata(\n",
    "        artifact_name=\"predictions\",\n",
    "        metadata={\n",
    "            \"prediction_info\": {\n",
    "                \"shape\": predictions.shape,\n",
    "                \"unique_values\": np.unique(predictions).tolist(),\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    log_artifact_metadata(\n",
    "        artifact_name=\"probabilities\",\n",
    "        metadata={\n",
    "            \"probability_info\": {\n",
    "                \"shape\": probabilities.shape,\n",
    "                \"min\": float(np.min(probabilities)),\n",
    "                \"max\": float(np.max(probabilities)),\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.svm import SVC\n",
    "from typing_extensions import Annotated\n",
    "from zenml import log_artifact_metadata, step\n",
    "\n",
    "\n",
    "class SHAPVisualization:\n",
    "    def __init__(self, shap_values, feature_names):\n",
    "        self.shap_values = shap_values\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "\n",
    "@step\n",
    "def explain_model(\n",
    "    model: SVC, X_train: pd.DataFrame\n",
    ") -> Annotated[SHAPVisualization, \"shap_visualization\"]:\n",
    "    \"\"\"Generate SHAP values for model explainability and create a visualization.\"\"\"\n",
    "    explainer = shap.KernelExplainer(\n",
    "        model.predict_proba, shap.sample(X_train, 100)\n",
    "    )\n",
    "    shap_values = explainer.shap_values(X_train.iloc[:100])\n",
    "\n",
    "    log_artifact_metadata(\n",
    "        artifact_name=\"shap_values\",\n",
    "        metadata={\n",
    "            \"shap_info\": {\n",
    "                \"shape\": [arr.shape for arr in shap_values],\n",
    "                \"n_classes\": len(shap_values),\n",
    "                \"n_features\": shap_values[0].shape[1],\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return SHAPVisualization(shap_values, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "from typing_extensions import Annotated\n",
    "from zenml import log_artifact_metadata, step\n",
    "\n",
    "\n",
    "@step\n",
    "def detect_data_drift(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> Annotated[Dict[str, float], \"drift_metrics\"]:\n",
    "    \"\"\"Detect data drift between training and test sets.\"\"\"\n",
    "    drift_metrics = {}\n",
    "    for column in X_train.columns:\n",
    "        _, p_value = ks_2samp(X_train[column], X_test[column])\n",
    "        drift_metrics[column] = p_value\n",
    "\n",
    "    log_artifact_metadata(\n",
    "        artifact_name=\"drift_metrics\",\n",
    "        metadata={\n",
    "            \"drift_summary\": {\n",
    "                \"high_drift_features\": [\n",
    "                    col for col, p in drift_metrics.items() if p < 0.05\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return drift_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import Model, pipeline\n",
    "\n",
    "\n",
    "@pipeline(\n",
    "    settings={\n",
    "        # \"docker\": DockerSettings(python_package_installer=\"uv\",\n",
    "        # requirements=\"requirements.txt\"),\n",
    "        # \"resources\": ResourceSettings(memory=\"8GB\"),\n",
    "    },\n",
    "    model=Model(name=\"high_risk_classification\"),\n",
    ")\n",
    "def iris_classification_pipeline():\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    model = train_model(X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    explain_model(model, X_train)\n",
    "    drift_metrics = detect_data_drift(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36miris_classification_pipeline\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mArchiving notebook code...\u001b[0m\n",
      "\u001b[1;35mCode already exists in artifact store, skipping upload.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "pipeline_run = iris_classification_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
