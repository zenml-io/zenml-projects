{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ab391a",
   "metadata": {},
   "source": [
    "# Intro to MLOps using ZenML\n",
    "\n",
    "## üåç Overview\n",
    "\n",
    "This repository is a minimalistic MLOps project intended as a starting point to learn how to put ML workflows in production. It features: \n",
    "\n",
    "- A feature engineering pipeline that loads data and prepares it for training.\n",
    "- A training pipeline that loads the preprocessed dataset and trains a model.\n",
    "- A batch inference pipeline that runs predictions on the trained model with new data.\n",
    "\n",
    "Follow along this notebook to understand how you can use ZenML to productionalize your ML workflows!\n",
    "\n",
    "<img src=\"_assets/pipeline_overview.png\" width=\"50%\" alt=\"Pipelines Overview\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b474",
   "metadata": {},
   "source": [
    "# ‚åö Step 2: Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87909827",
   "metadata": {},
   "source": [
    "Lets run the training pipeline\n",
    "\n",
    "<img src=\"_assets/training_pipeline.png\" width=\"50%\" alt=\"Training pipeline\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py --training-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a00008",
   "metadata": {},
   "source": [
    "Our two training steps both return different kinds of `sklearn` classifier\n",
    "models, so we use the generic `ClassifierMixin` type hint for the return type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f22174",
   "metadata": {},
   "source": [
    "ZenML allows you to load any version of any dataset that is tracked by the framework\n",
    "directly into a pipeline using the `ExternalArtifact` interface. This is very convenient\n",
    "in this case, as we'd like to send our preprocessed dataset from the older pipeline directly\n",
    "into the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa98f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def training(\n",
    "    train_dataset_id: Optional[UUID] = None,\n",
    "    test_dataset_id: Optional[UUID] = None,\n",
    "    model_type: str = \"sgd\",\n",
    "    min_train_accuracy: float = 0.0,\n",
    "    min_test_accuracy: float = 0.0,\n",
    "):\n",
    "    \"\"\"Model training pipeline.\"\"\" \n",
    "    if train_dataset_id is None or test_dataset_id is None:\n",
    "        # If we dont pass the IDs, this will run the feature engineering pipeline   \n",
    "        dataset_trn, dataset_tst = feature_engineering()\n",
    "    else:\n",
    "        # Load the datasets from an older pipeline\n",
    "        dataset_trn = ExternalArtifact(id=train_dataset_id)\n",
    "        dataset_tst = ExternalArtifact(id=test_dataset_id) \n",
    "\n",
    "    trained_model = model_trainer(\n",
    "        dataset_trn=dataset_trn,\n",
    "        model_type=model_type,\n",
    "    )\n",
    "\n",
    "    model_evaluator(\n",
    "        model=trained_model,\n",
    "        dataset_trn=dataset_trn,\n",
    "        dataset_tst=dataset_tst,\n",
    "        min_train_accuracy=min_train_accuracy,\n",
    "        min_test_accuracy=min_test_accuracy,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b70fd3",
   "metadata": {},
   "source": [
    "The end goal of this quick baseline evaluation is to understand which of the two\n",
    "models performs better. We'll use the `evaluator` step to compare the two\n",
    "models. This step takes in the model from the trainer step, and computes its score\n",
    "over the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64885ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random forest model with the chosen datasets.\n",
    "# We need to pass the ID's of the datasets into the function\n",
    "training(\n",
    "    model_type=\"rf\",\n",
    "    train_dataset_id=dataset_trn_artifact_version.id,\n",
    "    test_dataset_id=dataset_tst_artifact_version.id\n",
    ")\n",
    "\n",
    "rf_run = client.get_pipeline(\"training\").last_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a SGD classifier\n",
    "sgd_run = training(\n",
    "    model_type=\"sgd\",\n",
    "    train_dataset_id=dataset_trn_artifact_version.id,\n",
    "    test_dataset_id=dataset_tst_artifact_version.id\n",
    ")\n",
    "\n",
    "sgd_run = client.get_pipeline(\"training\").last_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1a68a",
   "metadata": {},
   "source": [
    "You can see from the logs already how our model training went: the\n",
    "`RandomForestClassifier` performed considerably better than the `SGDClassifier`.\n",
    "We can use the ZenML `Client` to verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95810b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluator returns a float value with the accuracy\n",
    "rf_run.steps[\"model_evaluator\"].output.load() > sgd_run.steps[\"model_evaluator\"].output.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256d145",
   "metadata": {},
   "source": [
    "# üíØ Step 3: Associating a model with your pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927978f3",
   "metadata": {},
   "source": [
    "You can see it is relatively easy to train ML models using ZenML pipelines. But it can be somewhat clunky to track\n",
    "all the models produced as you develop your experiments and use-cases. Luckily, ZenML offers a *Model Control Plane*,\n",
    "which is a central register of all your ML models.\n",
    "\n",
    "You can easily create a ZenML `Model` and associate it with your pipelines using the `ModelVersion` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_settings = {}\n",
    "\n",
    "# Lets add some metadata to the model to make it identifiable\n",
    "pipeline_settings[\"model_version\"] = ModelVersion(\n",
    "    name=\"breast_cancer_classifier\",\n",
    "    license=\"Apache 2.0\",\n",
    "    description=\"A breast cancer classifier\",\n",
    "    tags=[\"breast_cancer\", \"classifier\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the SGD model and set the version name to \"sgd\"\n",
    "pipeline_settings[\"model_version\"].version = \"sgd\"\n",
    "\n",
    "# the `with_options` method allows us to pass in pipeline settings\n",
    "#  and returns a configured pipeline\n",
    "training_configured = training.with_options(**pipeline_settings)\n",
    "\n",
    "# We can now run this as usual\n",
    "training_configured(\n",
    "    model_type=\"sgd\",\n",
    "    train_dataset_id=dataset_trn_artifact_version.id,\n",
    "    test_dataset_id=dataset_tst_artifact_version.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the RF model and set the version name to \"rf\"\n",
    "pipeline_settings[\"model_version\"].version = \"rf\"\n",
    "\n",
    "# the `with_options` method allows us to pass in pipeline settings\n",
    "#  and returns a configured pipeline\n",
    "training_configured = training.with_options(**pipeline_settings)\n",
    "\n",
    "# Let's run it again to make sure we have two versions\n",
    "training_configured(\n",
    "    model_type=\"rf\",\n",
    "    train_dataset_id=dataset_trn_artifact_version.id,\n",
    "    test_dataset_id=dataset_tst_artifact_version.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09597223",
   "metadata": {},
   "source": [
    "This time, running both pipelines has created two associated **model versions**.\n",
    "You can list your ZenML model and their versions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb25913",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenml_model = client.get_model(\"breast_cancer_classifier\")\n",
    "print(zenml_model)\n",
    "\n",
    "print(f\"Model {zenml_model.name} has {len(zenml_model.versions)} versions\")\n",
    "\n",
    "zenml_model.versions[0].version, zenml_model.versions[1].version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cfac2",
   "metadata": {},
   "source": [
    "The interesting part is that ZenML went ahead and linked all artifacts produced by the\n",
    "pipelines to that model version, including the two pickle files that represent our\n",
    "SGD and RandomForest classifier. We can see all artifacts directly from the model\n",
    "version object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31211413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the RF version\n",
    "rf_zenml_model_version = client.get_model_version(\"breast_cancer_classifier\", \"rf\")\n",
    "\n",
    "# We can now load our classifier directly as well\n",
    "random_forest_classifier = rf_zenml_model_version.get_artifact(\"sklearn_classifier\").load()\n",
    "\n",
    "random_forest_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53517a9a",
   "metadata": {},
   "source": [
    "If you are a [ZenML Cloud](https://zenml.io/cloud) user, you can see all of this visualized in the dashboard:\n",
    "\n",
    "<img src=\".assets/cloud_mcp_screenshot.png\" width=\"70%\" alt=\"Model Control Plane\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb645dde",
   "metadata": {},
   "source": [
    "There is a lot more you can do with ZenML models, including the ability to\n",
    "track metrics by adding metadata to it, or having them persist in a model\n",
    "registry. However, these topics can be explored more in the\n",
    "[ZenML docs](https://docs.zenml.io).\n",
    "\n",
    "For now, we will use the ZenML model control plane to promote our best\n",
    "model to `production`. You can do this by simply setting the `stage` of\n",
    "your chosen model version to the `production` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b718f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our best classifier to production\n",
    "rf_zenml_model_version.set_stage(\"production\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddf3d0",
   "metadata": {},
   "source": [
    "Of course, normally one would only promote the model by comparing to all other model\n",
    "versions and doing some other tests. But that's a bit more advanced use-case. See the\n",
    "[e2e_batch example](https://github.com/zenml-io/zenml/tree/main/examples/e2e) to get\n",
    "more insight into that sort of flow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecbc8cf",
   "metadata": {},
   "source": [
    "<img src=\".assets/cloud_mcp.png\" width=\"60%\" alt=\"Model Control Plane\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1146db",
   "metadata": {},
   "source": [
    "Once the model is promoted, we can now consume the right model version in our\n",
    "batch inference pipeline directly. Let's see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6306f14",
   "metadata": {},
   "source": [
    "# ü´Ö Step 4: Consuming the model in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f3108",
   "metadata": {},
   "source": [
    "The batch inference pipeline simply takes the model marked as `production` and runs inference on it\n",
    "with `live data`. The critical step here is the `inference_predict` step, where we load the model in memory\n",
    "and generate predictions:\n",
    "\n",
    "<img src=\".assets/inference_pipeline.png\" width=\"45%\" alt=\"Inference pipeline\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def inference_predict(dataset_inf: pd.DataFrame) -> Annotated[pd.Series, \"predictions\"]:\n",
    "    \"\"\"Predictions step\"\"\"\n",
    "    # Get the model_version\n",
    "    model_version = get_step_context().model_version\n",
    "\n",
    "    # run prediction from memory\n",
    "    predictor = model_version.load_artifact(\"sklearn_classifier\")\n",
    "    predictions = predictor.predict(dataset_inf)\n",
    "\n",
    "    predictions = pd.Series(predictions, name=\"predicted\")\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb227b",
   "metadata": {},
   "source": [
    "Apart from the loading the model, we must also load the preprocessing pipeline that we ran in feature engineering,\n",
    "so that we can do the exact steps that we did on training time, in inference time. Let's bring it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c409bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def inference(preprocess_pipeline_id: UUID):\n",
    "    \"\"\"Model batch inference pipeline\"\"\"\n",
    "    # random_state = client.get_artifact_version(id=preprocess_pipeline_id).metadata[\"random_state\"].value\n",
    "    # target = client.get_artifact_version(id=preprocess_pipeline_id).run_metadata['target'].value\n",
    "    random_state = 42\n",
    "    target = \"target\"\n",
    "\n",
    "    df_inference = data_loader(\n",
    "        random_state=random_state, is_inference=True\n",
    "    )\n",
    "    df_inference = inference_preprocessor(\n",
    "        dataset_inf=df_inference,\n",
    "        # We use the preprocess pipeline from the feature engineering pipeline\n",
    "        preprocess_pipeline=ExternalArtifact(id=preprocess_pipeline_id),\n",
    "        target=target,\n",
    "    )\n",
    "    inference_predict(\n",
    "        dataset_inf=df_inference,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afe7be",
   "metadata": {},
   "source": [
    "The way to load the right model is to pass in the `production` stage into the `ModelVersion` config this time.\n",
    "This will ensure to always load the production model, decoupled from all other pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_settings = {\"enable_cache\": False}\n",
    "\n",
    "# Lets add some metadata to the model to make it identifiable\n",
    "pipeline_settings[\"model_version\"] = ModelVersion(\n",
    "    name=\"breast_cancer_classifier\",\n",
    "    version=\"production\", # We can pass in the stage name here!\n",
    "    license=\"Apache 2.0\",\n",
    "    description=\"A breast cancer classifier\",\n",
    "    tags=[\"breast_cancer\", \"classifier\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3402f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `with_options` method allows us to pass in pipeline settings\n",
    "#  and returns a configured pipeline\n",
    "inference_configured = inference.with_options(**pipeline_settings)\n",
    "\n",
    "# Let's run it again to make sure we have two versions\n",
    "# We need to pass in the ID of the preprocessing done in the feature engineering pipeline\n",
    "# in order to avoid training-serving skew\n",
    "inference_configured(\n",
    "    preprocess_pipeline_id=preprocessing_pipeline_artifact_version.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2935d1fa",
   "metadata": {},
   "source": [
    "ZenML automatically links all artifacts to the `production` model version as well, including the predictions\n",
    "that were returned in the pipeline. This completes the MLOps loop of training to inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch production model\n",
    "production_model_version = client.get_model_version(\"breast_cancer_classifier\", \"production\")\n",
    "\n",
    "# Get the predictions artifact\n",
    "production_model_version.get_artifact(\"predictions\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a73cdf",
   "metadata": {},
   "source": [
    "You can also see all predictions ever created as a complete history in the dashboard:\n",
    "\n",
    "<img src=\".assets/cloud_mcp_predictions.png\" width=\"70%\" alt=\"Model Control Plane\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee4fc-f102-4b99-bdc3-2f1670c87679",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You're a legit MLOps engineer now! You trained two models, evaluated them against\n",
    "a test set, registered the best one with the ZenML model control plane,\n",
    "and served some predictions. You also learned how to iterate on your models and\n",
    "data by using some of the ZenML utility abstractions. You saw how to view your\n",
    "artifacts and models via the client as well as the ZenML Dashboard.\n",
    "\n",
    "## Further exploration\n",
    "\n",
    "This was just the tip of the iceberg of what ZenML can do; check out the [**docs**](https://docs.zenml.io/) to learn more\n",
    "about the capabilities of ZenML. For example, you might want to:\n",
    "\n",
    "- [Deploy ZenML](https://docs.zenml.io/user-guide/production-guide/connect-deployed-zenml) to collaborate with your colleagues.\n",
    "- Run the same pipeline on a [cloud MLOps stack in production](https://docs.zenml.io/user-guide/production-guide/cloud-stack).\n",
    "- Track your metrics in an experiment tracker like [MLflow](https://docs.zenml.io/stacks-and-components/component-guide/experiment-trackers/mlflow).\n",
    "\n",
    "## What next?\n",
    "\n",
    "* If you have questions or feedback... join our [**Slack Community**](https://zenml.io/slack) and become part of the ZenML family!\n",
    "* If you want to quickly get started with ZenML, check out the [ZenML Cloud](https://zenml.io/cloud)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
