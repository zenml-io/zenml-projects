{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Personalized AI with Flux.1 and DreamBooth: A Step-by-Step Guide\n",
    "\n",
    "Hey there, AI enthusiasts! üëã Ready to dive into the wild world of personalized AI models? Buckle up, because we're about to embark on an epic journey to create a system that can handle thousands of personalized Flux.1 model finetunings like it's no big deal. We'll be using the awesome power of DreamBooth and some nifty open-source tools like ZenML to make this magic happen.\n",
    "\n",
    "By the time you're done with this notebook, you'll be slinging personalized AI models like a pro. Whether you're a seasoned ML wizard or a curious newbie, this guide will give you the superpowers you need to bring these ideas to life in your own mad scientist projects. Let's get this party started! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up Our Environment\n",
    "\n",
    "First things first, let's get our environment ready for some serious AI action. We'll import all the necessary libraries and set up our configuration classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from typing import Annotated, List, Tuple\n",
    "\n",
    "import torch\n",
    "from accelerate.utils import write_basic_config\n",
    "from diffusers import AutoPipelineForText2Image, StableVideoDiffusionPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "from PIL import Image as PILImage\n",
    "from rich import print\n",
    "from train_dreambooth_lora_flux import main as dreambooth_main\n",
    "from zenml import pipeline, step\n",
    "from zenml.config import DockerSettings\n",
    "from zenml.integrations.huggingface.steps import run_with_accelerate\n",
    "from zenml.integrations.kubernetes.flavors import (\n",
    "    KubernetesOrchestratorSettings,\n",
    ")\n",
    "from zenml.logger import get_logger\n",
    "from zenml.types import HTMLString\n",
    "from zenml.utils import io_utils\n",
    "from zenml.client import Client\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "MNT_PATH = \"/mnt/data\"\n",
    "\n",
    "docker_settings = DockerSettings(\n",
    "    parent_image=\"pytorch/pytorch:2.2.2-cuda11.8-cudnn8-runtime\",\n",
    "    environment={\n",
    "        \"PJRT_DEVICE\": \"CUDA\",\n",
    "        \"USE_TORCH_XLA\": \"false\",\n",
    "        \"MKL_SERVICE_FORCE_INTEL\": 1,\n",
    "        \"HF_TOKEN\": os.environ[\"HF_TOKEN\"],\n",
    "        \"HF_HOME\": MNT_PATH,\n",
    "    },\n",
    "    python_package_installer=\"uv\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    python_package_installer_args={\n",
    "        \"system\": None,\n",
    "    },\n",
    "    apt_packages=[\"git\", \"ffmpeg\", \"gifsicle\"],\n",
    "    # prevent_build_reuse=True,\n",
    ")\n",
    "\n",
    "kubernetes_settings = KubernetesOrchestratorSettings(\n",
    "    pod_settings={\n",
    "        \"affinity\": {\n",
    "            \"nodeAffinity\": {\n",
    "                \"requiredDuringSchedulingIgnoredDuringExecution\": {\n",
    "                    \"nodeSelectorTerms\": [\n",
    "                        {\n",
    "                            \"matchExpressions\": [\n",
    "                                {\n",
    "                                    \"key\": \"zenml.io/gpu\",\n",
    "                                    \"operator\": \"In\",\n",
    "                                    \"values\": [\"yes\"],\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"volumes\": [\n",
    "            {\n",
    "                \"name\": \"data-volume\",\n",
    "                \"persistentVolumeClaim\": {\"claimName\": \"pvc-managed-premium\"},\n",
    "            }\n",
    "        ],\n",
    "        \"volume_mounts\": [{\"name\": \"data-volume\", \"mountPath\": MNT_PATH}],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Environment setup complete! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading Magic\n",
    "\n",
    "Now that we've got our environment set up, let's create a function to load our training data. This bad boy will help us grab all those juicy image paths we'll use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCould not import AWS service connector: No module named 'boto3'.\u001b[0m\n",
      "\u001b[33mCould not import GCP service connector: cannot import name 'artifactregistry_v1' from 'google.cloud' (unknown location).\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n",
      "\u001b[1;35mClientSecretCredential.get_token succeeded\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from zenml.client import Client\n",
    "from zenml.utils import io_utils\n",
    "\n",
    "images_path = \"az://demo-zenmlartifactstore/hamza-faces\"\n",
    "images_dir_path = \"/tmp/hamza-faces/\"\n",
    "_ = Client().active_stack.artifact_store.path\n",
    "\n",
    "io_utils.copy_dir(\n",
    "    destination_dir=images_dir_path,\n",
    "    source_dir=images_path,\n",
    "    overwrite=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'on_click'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     display(thumbnail_grid, selected_image)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mdisplay_image_gallery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_dir_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mdisplay_image_gallery\u001b[0;34m(images_dir_path, thumbnail_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Add click event to thumbnails\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m thumbnail \u001b[38;5;129;01min\u001b[39;00m thumbnails:\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mthumbnail\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_click\u001b[49m(on_thumbnail_click)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Display the gallery\u001b[39;00m\n\u001b[1;32m     37\u001b[0m display(thumbnail_grid, selected_image)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'on_click'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def display_image_gallery(images_dir_path, thumbnail_size=(200, 200)):\n",
    "    # Get all image files from the directory\n",
    "    image_files = [f for f in os.listdir(images_dir_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # Create thumbnail widgets\n",
    "    thumbnails = [widgets.Image(\n",
    "        value=open(os.path.join(images_dir_path, img), 'rb').read(),\n",
    "        format=img.split('.')[-1],\n",
    "        layout=widgets.Layout(width=f'{thumbnail_size[0]}px', height=f'{thumbnail_size[1]}px', margin='5px')\n",
    "    ) for img in image_files]\n",
    "    \n",
    "    # Create buttons for thumbnails\n",
    "    buttons = [widgets.Button(layout=widgets.Layout(width=f'{thumbnail_size[0]}px', height=f'{thumbnail_size[1]}px', padding='0')) for _ in thumbnails]\n",
    "    \n",
    "    # Add thumbnails as button icons\n",
    "    for thumbnail, button in zip(thumbnails, buttons):\n",
    "        button.icon = thumbnail\n",
    "    \n",
    "    # Create a grid of buttons\n",
    "    thumbnail_grid = widgets.GridBox(\n",
    "        buttons,\n",
    "        layout=widgets.Layout(grid_template_columns=\"repeat(auto-fill, minmax(200px, 1fr))\")\n",
    "    )\n",
    "    \n",
    "    # Create a larger image widget for the selected image\n",
    "    selected_image = widgets.Image(\n",
    "        layout=widgets.Layout(max_width='100%', height='auto', margin='10px 0')\n",
    "    )\n",
    "    \n",
    "    # Function to update the selected image\n",
    "    def on_button_click(button):\n",
    "        index = buttons.index(button)\n",
    "        selected_image.value = thumbnails[index].value\n",
    "        selected_image.format = thumbnails[index].format\n",
    "    \n",
    "    # Add click event to buttons\n",
    "    for button in buttons:\n",
    "        button.on_click(on_button_click)\n",
    "    \n",
    "    # Display the gallery\n",
    "    display(thumbnail_grid, selected_image)\n",
    "\n",
    "\n",
    "# Usage\n",
    "display_image_gallery(images_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_paths(image_dir: Path) -> List[Path]:\n",
    "    logger.info(f\"Loading images from {image_dir}\")\n",
    "    return (\n",
    "        list(image_dir.glob(\"**/*.png\"))\n",
    "        + list(image_dir.glob(\"**/*.jpg\"))\n",
    "        + list(image_dir.glob(\"**/*.jpeg\"))\n",
    "    )\n",
    "\n",
    "@step(\n",
    "    settings={\"orchestrator.kubernetes\": kubernetes_settings},\n",
    "    enable_cache=False,\n",
    ")\n",
    "def load_data(instance_example_dir: str) -> List[PILImage.Image]:\n",
    "    instance_example_paths = load_image_paths(Path(instance_example_dir))\n",
    "    logger.info(f\"Loaded {len(instance_example_paths)} images\")\n",
    "    return [PILImage.open(path) for path in instance_example_paths]\n",
    "\n",
    "print(\"Data loading function ready to roll! üì∏\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training Our Model Like a Boss\n",
    "\n",
    "Alright, now we're getting to the good stuff. Let's set up our model training step. This is where the magic happens, folks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_with_accelerate(num_processes=1, multi_gpu=True)\n",
    "@step(\n",
    "    settings={\"orchestrator.kubernetes\": kubernetes_settings},\n",
    "    enable_cache=False,\n",
    ")\n",
    "def train_model(\n",
    "    instance_example_images: List[PILImage.Image],\n",
    "    instance_name: str,\n",
    "    class_name: str,\n",
    "    model_name: str,\n",
    "    hf_repo_suffix: str,\n",
    "    prefix: str,\n",
    "    resolution: int,\n",
    "    train_batch_size: int,\n",
    "    rank: int,\n",
    "    gradient_accumulation_steps: int,\n",
    "    learning_rate: float,\n",
    "    lr_scheduler: str,\n",
    "    lr_warmup_steps: int,\n",
    "    max_train_steps: int,\n",
    "    push_to_hub: bool,\n",
    "    checkpointing_steps: int,\n",
    "    seed: int,\n",
    ") -> None:\n",
    "    # Set up a temporary directory for our images\n",
    "    image_dir = Path(tempfile.mkdtemp(prefix=\"instance_images_\"))\n",
    "    for i, image in enumerate(instance_example_images):\n",
    "        image.save(image_dir / f\"image_{i}.png\")\n",
    "\n",
    "    logger.info(f\"Saved images to {image_dir}\")\n",
    "    images_dir_path = str(image_dir)\n",
    "\n",
    "    # Configure accelerate for some speedy training\n",
    "    write_basic_config(mixed_precision=\"bf16\")\n",
    "\n",
    "    instance_phrase = f\"{instance_name} the {class_name}\"\n",
    "    instance_prompt = f\"{prefix} {instance_phrase}\".strip()\n",
    "\n",
    "    # Set up our training arguments\n",
    "    class Args:\n",
    "        def __init__(self, **kwargs):\n",
    "            self.__dict__.update(kwargs)\n",
    "\n",
    "    args = Args(\n",
    "        mixed_precision=\"bf16\",\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "        instance_data_dir=images_dir_path,\n",
    "        output_dir=hf_repo_suffix,\n",
    "        instance_prompt=instance_prompt,\n",
    "        resolution=resolution,\n",
    "        train_batch_size=train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        rank=rank,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        lr_warmup_steps=lr_warmup_steps,\n",
    "        max_train_steps=max_train_steps,\n",
    "        checkpointing_steps=checkpointing_steps,\n",
    "        seed=seed,\n",
    "        push_to_hub=push_to_hub if push_to_hub else \"\",\n",
    "    )\n",
    "\n",
    "    # Fire up that training engine!\n",
    "    print(\"Launching dreambooth training script\")\n",
    "    dreambooth_main(args)\n",
    "\n",
    "print(\"Model training step locked and loaded! üí™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Batch Inference - Let's See What We've Created!\n",
    "\n",
    "Now that we've trained our model, it's time to put it to the test. Let's set up a batch inference step to generate some cool images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(settings={\"orchestrator.kubernetes\": kubernetes_settings})\n",
    "def batch_inference(\n",
    "    hf_username: str,\n",
    "    hf_repo_suffix: str,\n",
    "    instance_name: str,\n",
    "    class_name: str,\n",
    ") -> PILImage.Image:\n",
    "    model_path = f\"{hf_username}/{hf_repo_suffix}\"\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16\n",
    "    ).to(\"cuda\")\n",
    "    pipe.load_lora_weights(\n",
    "        model_path, weight_name=\"pytorch_lora_weights.safetensors\"\n",
    "    )\n",
    "\n",
    "    instance_phrase = f\"{instance_name} the {class_name}\"\n",
    "    prompts = [\n",
    "        f\"A photo of {instance_phrase} wearing a beret in front of the Eiffel Tower\",\n",
    "        f\"A photo of {instance_phrase} on a busy Paris street\",\n",
    "        f\"A photo of {instance_phrase} sitting at a Parisian cafe\",\n",
    "        # ... (add more prompts as desired)\n",
    "    ]\n",
    "\n",
    "    images = pipe(\n",
    "        prompt=prompts,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        height=512,\n",
    "        width=512,\n",
    "    ).images\n",
    "\n",
    "    # Create a cool gallery image with all our generated pics\n",
    "    width, height = images[0].size\n",
    "    rows, cols = 3, 5\n",
    "    gallery_img = PILImage.new(\"RGB\", (width * cols, height * rows))\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        gallery_img.paste(image, ((i % cols) * width, (i // cols) * height))\n",
    "\n",
    "    return gallery_img\n",
    "\n",
    "print(\"Batch inference step ready to generate some masterpieces! üé®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: From Still to Motion - Let's Make Some Video Magic!\n",
    "\n",
    "Why stop at images when we can create videos? Let's add a step to turn our generated image into a short video clip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_size(\n",
    "    image: PILImage.Image, max_size: int = 1024\n",
    ") -> Tuple[int, int]:\n",
    "    width, height = image.size\n",
    "    aspect_ratio = width / height\n",
    "    if width > height:\n",
    "        new_width = min(width, max_size)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = min(height, max_size)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "    return (new_width, new_height)\n",
    "\n",
    "@step(\n",
    "    settings={\"orchestrator.kubernetes\": kubernetes_settings},\n",
    "    enable_cache=False,\n",
    ")\n",
    "def image_to_video(\n",
    "    hf_username: str,\n",
    "    hf_repo_suffix: str,\n",
    "    instance_name: str,\n",
    ") -> Tuple[\n",
    "    Annotated[PILImage.Image, \"generated_image\"],\n",
    "    Annotated[bytes, \"video_data\"],\n",
    "    Annotated[HTMLString, \"video_html\"],\n",
    "]:\n",
    "    model_path = f\"{hf_username}/{hf_repo_suffix}\"\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16\n",
    "    ).to(\"cuda\")\n",
    "    pipe.load_lora_weights(\n",
    "        model_path, weight_name=\"pytorch_lora_weights.safetensors\"\n",
    "    )\n",
    "\n",
    "    image = pipe(\n",
    "        prompt=f\"A photo of {instance_name} on a busy Paris street\",\n",
    "        num_inference_steps=70,\n",
    "        guidance_scale=7.5,\n",
    "        height=512,\n",
    "        width=512,\n",
    "    ).images[0]\n",
    "\n",
    "    video_pipeline = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "    )\n",
    "    video_pipeline.enable_model_cpu_offload()\n",
    "\n",
    "    optimal_size = get_optimal_size(image)\n",
    "    image = image.resize(optimal_size)\n",
    "    optimal_width, optimal_height = optimal_size\n",
    "\n",
    "    frames = video_pipeline(\n",
    "        image,\n",
    "        num_inference_steps=50,\n",
    "        decode_chunk_size=8,\n",
    "        generator=torch.manual_seed(42),\n",
    "        height=optimal_height,\n",
    "        width=optimal_width,\n",
    "    ).frames[0]\n",
    "\n",
    "    output_file = \"generated_video.mp4\"\n",
    "    export_to_video(frames, output_file, fps=7)\n",
    "\n",
    "    with open(output_file, \"rb\") as file:\n",
    "        video_data = file.read()\n",
    "\n",
    "    html_visualization_str = f\"\"\"\n",
    "    <html>\n",
    "    <body>\n",
    "        <video width=\"{optimal_width}\" height=\"{optimal_height}\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{base64.b64encode(video_data).decode()}\" type=\"video/mp4\">\n",
    "            Your browser does not support the video tag.\n",
    "        </video>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    return (image, video_data, HTMLString(html_visualization_str))\n",
    "\n",
    "print(\"Video generation step ready to bring your images to life! üé¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Putting It All Together - Our Dreambooth Pipeline\n",
    "\n",
    "Now for the grand finale - let's string all these awesome steps together into one epic pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(settings={\"docker\": docker_settings})\n",
    "def dreambooth_pipeline(\n",
    "    instance_example_dir: str = \"data/hamza-instance-images\",\n",
    "    instance_name: str = \"htahir1\",\n",
    "    class_name: str = \"Pakistani man\",\n",
    "    model_name: str = \"black-forest-labs/FLUX.1-dev\",\n",
    "    hf_username: str = \"htahir1\",\n",
    "    hf_repo_suffix: str = \"flux-dreambooth-hamza\",\n",
    "    prefix: str = \"A photo of\",\n",
    "    resolution: int = 512,\n",
    "    train_batch_size: int = 1,\n",
    "    rank: int = 16,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    learning_rate: float = 0.0004,\n",
    "    lr_scheduler: str = \"constant\",\n",
    "    lr_warmup_steps: int = 0,\n",
    "    max_train_steps: int = 1600,\n",
    "    push_to_hub: bool = True,\n",
    "    checkpointing_steps: int = 1000,\n",
    "    seed: int = 117,\n",
    "):\n",
    "    data = load_data(instance_example_dir)\n",
    "    train_model(\n",
    "        data,\n",
    "        instance_name=instance_name,\n",
    "        class_name=class_name,\n",
    "        model_name=model_name,\n",
    "        hf_repo_suffix=hf_repo_suffix,\n",
    "        prefix=prefix,\n",
    "        resolution=resolution,\n",
    "        train_batch_size=train_batch_size,\n",
    "        rank=rank,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        lr_warmup_steps=lr_warmup_steps,\n",
    "        max_train_steps=max_train_steps,\n",
    "        push_to_hub=push_to_hub,\n",
    "        checkpointing_steps=checkpointing_steps,\n",
    "        seed=seed,\n",
    "    )\n",
    "    batch_inference(\n",
    "        hf_username,\n",
    "        hf_repo_suffix,\n",
    "        instance_name,\n",
    "        class_name,\n",
    "        after=\"train_model\",\n",
    "    )\n",
    "    image_to_video(\n",
    "        hf_username, hf_repo_suffix, instance_name, after=\"batch_inference\"\n",
    "    )\n",
    "\n",
    "print(\"Dreambooth pipeline assembled and ready for action! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Launch the Pipeline and Watch the Magic Happen!\n",
    "\n",
    "Alright, folks, this is it - the moment of truth! Let's fire up our pipeline and see this baby in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dreambooth_pipeline()\n",
    "\n",
    "print(\"Pipeline launched! Sit back, relax, and prepare to be amazed! üçø\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it, folks! You've just built and launched a kickass pipeline for personalized AI model generation. From loading data to training models, from generating images to creating videos - you've done it all!\n",
    "\n",
    "Remember, this is just the beginning. Feel free to tweak, adjust, and experiment with the parameters to see what kind of magic you can create. The AI world is your oyster, and you've got the tools to make some serious pearls!\n",
    "\n",
    "Happy coding, and may your models be ever accurate and your latency low! üöÄüéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
